# Brophy (2025) — Wide Reflective Equilibrium in LLM Alignment: Bridging Moral Epistemology and AI Safety

## Bibliographic

- **Title:** Wide Reflective Equilibrium in LLM Alignment: Bridging Moral Epistemology and AI Safety
- **Authors:** Matthew Brophy
- **Date:** May 2025
- **Venue:** arXiv preprint
- **URL:** https://arxiv.org/abs/2506.00415
- **License:** Not specified
- **Pages:** ~25

## File

- **Local path:** `papers/downloads/arxiv-2506.00415.pdf`
- **SHA-256:** `8c4133144bb632f94138832c2449ba41e6e7dcb67b1aa276c327de5a2b0b6e20`
- **Size:** 478,208 bytes (approx)
- **Downloaded:** 2026-02-28

## Summary

Argues that the Method of Wide Reflective Equilibrium (MWRE) — seeking coherence among considered moral judgments, guiding moral principles, and relevant background theories — provides a philosophically rigorous framework for understanding and improving LLM alignment. The paper contends that current alignment techniques like Constitutional AI bear a structural resemblance to MWRE but lack its emphasis on dynamic, bidirectional revision of principles and the procedural legitimacy that follows. Proposes that explicitly adopting MWRE would enhance the normative credibility of alignment efforts.

## Relevance to Our Paper

**Medium-high.** Key connections:

1. **Structural parallel to our tenet-constraint approach** — Brophy imports philosophical methodology (MWRE) into AI alignment; we use AI systems to produce philosophy under explicit constraints. Both bridge moral epistemology and AI practice, from opposite directions.

2. **Supports our architecture's emphasis on revision** — MWRE's core feature is dynamic mutual adjustment between principles and judgments, which resonates with our continuous review cycles (pessimistic, optimistic, deep review) that can surface tensions between tenet commitments and generated content.

3. **Argues MWRE offers advantages over Constitutional AI** — specifically, the dynamic revisability of principles. Our system partially implements this: while our five tenets are fixed, the content generated under them is continuously revised through adversarial review.

## Notes

- Author is from the Department of Philosophy, High Point University
- Engages specifically with Anthropic's Constitutional AI as the primary case study
- Discusses the Claude Opus 4 red-teaming incident (May 2025 blackmail scenario) as evidence that surface-level alignment is insufficient
- Proposes three goals: descriptive fit (MWRE captures alignment logic), normative transfer (MWRE lends ethical justification), and critical insight (where the analogy breaks)
