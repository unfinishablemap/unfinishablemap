# Milliere & Buckner (2024) — A Philosophical Introduction to Language Models — Part II: The Way Forward

## Bibliographic

- **Title:** A Philosophical Introduction to Language Models — Part II: The Way Forward
- **Authors:** Raphael Milliere, Cameron Buckner
- **Date:** 6 May 2024
- **Venue:** arXiv preprint (cs.CL, cs.AI)
- **URL:** https://arxiv.org/abs/2405.03207
- **License:** Not specified
- **Pages:** ~55

## File

- **Local path:** `papers/downloads/arxiv-2405.03207.pdf`
- **SHA-256:** `8489718949867b2c20dbbe18690c65d7c549a6df0bb201d5eb9636abacc2ff6d`
- **Size:** 1,572,864 bytes (approx)
- **Downloaded:** 2026-02-28

## Summary

The second part of a two-part philosophical survey, turning from classical debates (covered in Part I) to novel questions raised by LLM progress. Focuses on three areas: mechanistic understanding via causal intervention methods that probe LLMs' internal representations beyond behavioral performance; near-horizon philosophical questions about multimodality, agency, consciousness, and reproducibility; and whether LLM-like systems may be relevant to modeling aspects of human and animal cognition. Highlights the limitations of benchmark-based evaluation, including saturation, gamification, contamination, and construct validity concerns.

## Relevance to Our Paper

**Medium.** Key connections:

1. **Consciousness discussion directly relevant to our tenets** — Part II's treatment of whether LLMs might meet minimal criteria for consciousness intersects with our Map's dualism tenet and its content on consciousness and physical processes.

2. **Benchmark critique motivates our evaluation approach** — the paper's arguments about construct validity problems in LLM benchmarks (Goodhart's Law, contamination, human-centric test assumptions) provide background for why we evaluate philosophical content quality through adversarial review rather than standardized benchmarks.

3. **Mechanistic interpretability section** — discussion of causal intervention methods for understanding LLM internals provides context for the broader question of whether LLM outputs reflect genuine reasoning or surface pattern matching, which bears on the credibility of AI-generated philosophical content.

## Notes

- Companion paper to Part I (arxiv-2401.03910)
- Both authors are philosophers (Macquarie University, University of Houston)
- Discusses Theory of Mind construct validity at length — arguing that LLM performance on ToM tasks does not necessarily indicate the capacity it measures in humans
- Approximately 55 pages with extensive bibliography spanning both philosophy and ML
