# Shanahan (2023) — Talking About Large Language Models

## Bibliographic

- **Title:** Talking About Large Language Models
- **Authors:** Murray Shanahan
- **Date:** December 2022 (revised February 2023)
- **Venue:** arXiv preprint (cs.CL)
- **URL:** https://arxiv.org/abs/2212.03551
- **License:** Not specified
- **Pages:** ~13

## File

- **Local path:** `papers/downloads/arxiv-2212.03551.pdf`
- **SHA-256:** `bed3a50fc04e0bedf02193e508be2e6415079dd74386c9a87481bffe185304de`
- **Size:** 190,464 bytes (approx)
- **Downloaded:** 2026-02-28

## Summary

Argues that the fluency of LLM output makes humans vulnerable to anthropomorphism, tempting us to describe these systems using philosophically loaded terms like "knows", "believes", and "thinks". The paper advocates repeatedly stepping back to consider what LLMs actually do — generate statistically likely word sequences — and urges scientific precision in how we talk about them. Shanahan draws on Wittgenstein and Dennett's intentional stance to distinguish between harmless shorthand and misleading attribution of mental states.

## Relevance to Our Paper

**Medium-high.** Important for framing our paper's language carefully:

1. **Directly motivates our careful framing** — we describe AI as *contributing to* philosophical knowledge production, not *doing* philosophy autonomously. Shanahan's argument is precisely why this distinction matters.

2. **The intentional stance discussion** is relevant to how we characterise what the Map's AI system does: it generates, reviews, and refines content, but we avoid claiming it "understands" or "believes" the tenets.

3. **Useful methodological guardrail** — citing Shanahan signals we are aware of the anthropomorphism trap and have designed our language accordingly.

## Notes

- One of the most widely cited papers on philosophical precision in LLM discourse
- Published before GPT-4; the anthropomorphism concerns have only intensified since
- Shanahan is at Imperial College London, a major figure in AI and philosophy of mind
- Distinct from Shanahan & Singler (#15), which examines extended conversations; this paper is about how we *talk about* LLMs
