# Literature Review — References

Working bibliography for "The Unfinishable Map: Continuous AI-Assisted Philosophical Inquiry Through Adversarial Self-Review"

## Downloaded Papers (manifests in `papers/manifests/`)

| ID | Authors | Title | Relevance |
|----|---------|-------|-----------|
| `chemrxiv-2025-rwxdk` | Harb et al. (2025) | Towards Philosophical Reasoning with Agentic LLMs: Socratic Method for Scientific Assistance | Low-med |
| `arxiv-2302.01339` | Schwitzgebel et al. (2023) | Creating a Large Language Model of a Philosopher | High |

## Category 1: AI-Assisted Knowledge Production

1. **Shao, Y., Jiang, Y., Kanell, T., Xu, P., Khattab, O., & Lam, M. (2024).** "Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models" (STORM). *NAACL 2024*. https://arxiv.org/abs/2402.14207
   - LLM system that writes Wikipedia-style articles via multi-perspective research conversations. Closest system-level comparison to the Map, but single-shot generation without continuous review.

2. **Jiang, Y., Shao, Y., et al. (2024).** "Into the Unknown Unknowns: Engaged Human Learning through Participation in Language Model Agent Conversations" (Co-STORM). *EMNLP 2024*. https://storm-project.stanford.edu/research/storm/
   - Extends STORM to collaborative human-AI knowledge curation. Relevant to our human-directed, AI-executed model.

3. **Liang, W., Zhang, Y., et al. (2025).** "The Widespread Adoption of Large Language Model-Assisted Writing Across Society." *Patterns (Cell Press)*. https://arxiv.org/abs/2502.09747
   - 10-24% of text in consumer complaints, corporate comms, and job postings shows LLM assistance. Context for how widespread AI writing has become.

4. **Brooks, C. & Eggert, S. (2024).** "The Rise of AI-Generated Content in Wikipedia." *WikiNLP 2024*. https://arxiv.org/abs/2410.08044
   - >5% of new English Wikipedia articles are AI-generated. Useful for positioning the Map's transparent attribution against undisclosed AI content.

## Category 2: Human-AI Co-Authorship and Attribution

5. **He, J., Houde, S., & Weisz, J.D. (2025).** "Which Contributions Deserve Credit? Perceptions of Attribution in Human-AI Co-Creation." *CHI 2025*. https://arxiv.org/abs/2502.18357
   - Survey (N=155) finding AI gets less credit than human partners for equivalent contributions. Directly relevant to our ai_contribution scoring system.

6. **COPE Position Statement (2023, updated 2024).** "Authorship and AI Tools." https://publicationethics.org/guidance/cope-position/authorship-and-ai-tools
   - Leading publication ethics body's position that AI cannot be listed as author. Background for our AI pseudonym approach.

7. **Kirchner, S. (2025).** "Generative AI Meets Knowledge Management." *Knowledge and Process Management (Wiley)*. https://doi.org/10.1002/kpm.70004
   - How AI-generated content is attributed and managed in collaborative work contexts.

## Category 3: Constrained / Constitutional AI

8. **Bai, Y., Kadavath, S., et al. (2022).** "Constitutional AI: Harmlessness from AI Feedback." *arXiv (Anthropic)*. https://arxiv.org/abs/2212.08073
   - **Seminal.** Principle-driven AI alignment using natural-language constitutions. Our tenets function analogously — explicit constraints that guide generation and self-review. Key conceptual precedent.

9. **Huang, S., Siddarth, D., et al. (2024).** "Collective Constitutional AI: Aligning a Language Model with Public Input." *FAccT 2024*. https://arxiv.org/abs/2406.07814
   - Constitutional principles sourced from public input rather than developers. Relevant to our tenet-as-constraint approach.

10. **Findeis, A., et al. (2024).** "Inverse Constitutional AI: Compressing Preferences into Principles." *arXiv*. https://arxiv.org/abs/2406.06560
    - Extracts implicit principles from preference data. Inverse of our approach (we start with explicit principles). Useful contrast.

25. **Brophy, M. (2025).** "Wide Reflective Equilibrium in LLM Alignment: Bridging Moral Epistemology and AI Safety." *arXiv*. https://arxiv.org/abs/2506.00415
    - Proposes using Wide Reflective Equilibrium (coherence between judgments, principles, and background theories) to improve LLM alignment. Structural parallel to our tenet-constraint approach: they import philosophical methodology into AI systems; we use AI systems to produce philosophy under explicit constraints. Argues MWRE offers advantages over Constitutional AI by promoting dynamic revision.

## Category 3b: Philosophy of Science (Structural Analogies)

34. **Lakatos, I. (1978).** *The Methodology of Scientific Research Programmes: Philosophical Papers Volume 1.* Cambridge University Press.
    - Introduces the framework of scientific research programmes: a "hard core" of irrefutable commitments surrounded by a "protective belt" of auxiliary hypotheses subject to testing and revision. We draw a structural analogy: our tenets function as the hard core, the article corpus as the protective belt, and review layers as critical testing. Convergence caps correspond to programme maturation. This is a structural parallel, not a claim that the Map constitutes a research programme in Lakatos' technical sense.

## Category 4: LLM-Based Philosophical Reasoning

11. **Schwitzgebel, E., Schwitzgebel, D., & Strasser, A. (2023).** "Creating a Large Language Model of a Philosopher." *arXiv*. https://arxiv.org/abs/2302.01339
    - **Key paper.** Fine-tuned GPT-3 on Dennett's works; experts struggled to distinguish outputs. Establishes LLMs can produce credible philosophical text. Our work extends this from mimicry to original, constrained, continuously-improved content. [Downloaded]

12. **Harb, H., Sun, Y., Unal, M., et al. (2025).** "Towards Philosophical Reasoning with Agentic LLMs: Socratic Method for Scientific Assistance." *ChemRxiv / ML: Science and Technology*. https://doi.org/10.26434/chemrxiv-2025-rwxdk
    - Socratic principles structure LLM prompting for scientific reasoning. Inverted relation to our work: they use philosophy to improve LLM science; we use LLMs to produce philosophy. [Downloaded]

13. **Milliere, R. & Buckner, C. (2024).** "A Philosophical Introduction to Language Models — Part I: Continuity With Classic Debates" and "Part II: The Way Forward." *arXiv*. Part I: https://arxiv.org/abs/2401.03910 | Part II: https://arxiv.org/abs/2405.03207
    - Comprehensive philosophical treatment of LLMs, covering compositionality, grounding, world models, consciousness. Background for positioning our work.

14. **Chalmers, D.J. (2023).** "Could a Large Language Model be Conscious?" *Boston Review / NeurIPS 2022 invited talk*. https://arxiv.org/abs/2303.07103
    - Prominent philosopher on LLM consciousness. Our Map's tenets engage directly with these questions. Useful for framing.

15. **Shanahan, M. & Singler, B. (2024).** "Existential Conversations with Large Language Models." *arXiv*. https://arxiv.org/abs/2411.13223
    - Examines extended philosophical conversations with LLMs. Related to the Map's approach of sustained philosophical engagement.

31. **Shanahan, M. (2023).** "Talking About Large Language Models." *arXiv*. https://arxiv.org/abs/2212.03551
    - Warns that fluent LLM output increases anthropomorphic misinterpretation; urges precision about what LLMs actually do vs. philosophically loaded descriptions ("knows", "believes", "concludes"). Important for our paper's careful framing: we describe AI as *contributing to* philosophical knowledge production, not *doing* philosophy autonomously. Distinct from Shanahan & Singler (#15), which examines extended conversations.

16. **Guo, Z. (2025).** "Meta-Analysis of LLM Performance on Philosophical Questions." *ResearchGate preprint*.
    - Evaluates GPT-4, Claude, etc. on philosophical reasoning. Finds LLMs strong on information but weak on "wisdom-level" reasoning.

26. **Goldstein, S. (2024).** "LLMs Can Never Be Ideally Rational." *PhilArchive preprint*. https://philarchive.org/rec/GOLLCN
    - Argues LLMs have fundamental architectural limits as rational agents — next-word prediction produces incoherent probabilistic judgments and intransitive preferences. Motivates our design: if raw LLM output is inherently unreliable, systematic constraints (tenets) and adversarial review layers are architecturally necessary, not optional.

27. **Gage, L. (2025).** "A Consequentialist Defense of AI-Assisted Philosophical Discovery." *PhilArchive preprint*. https://philarchive.org/rec/GAGACD
    - **Key paper.** Defends "Augmented Agency" — human conceptual architect + AI as syntactic/ideation engine — as a legitimate method of philosophical discovery. Argues philosophical ideas should be evaluated on intellectual merit, not discoverer's credentials. Directly supports our project's framing: human-directed, AI-executed philosophical inquiry. Treat as a position in metaphilosophy, not settled consensus.

28. **Xu, Z., Jain, S., & Kankanhalli, M. (2024).** "Hallucination is Inevitable: An Innate Limitation of Large Language Models." *arXiv*. https://arxiv.org/abs/2401.11817
    - Formal proof that hallucination cannot be eliminated in LLMs — a mathematical inevitability, not just an engineering problem. Directly motivates our multi-layer adversarial review architecture: if no single generation pass can be hallucination-free, systematic post-generation review is structurally required.

## Category 5: Adversarial Self-Review / Self-Critique

17. **Madaan, A., Tandon, N., et al. (2023).** "Self-Refine: Iterative Refinement with Self-Feedback." *NeurIPS 2023*. https://arxiv.org/abs/2303.17651
    - **Seminal.** Same LLM generates, critiques, and refines its own output. Our review layers extend this principle to a persistent knowledge base rather than individual responses.

18. **Shinn, N., Cassano, F., et al. (2023).** "Reflexion: Language Agents with Verbal Reinforcement Learning." *NeurIPS 2023*. https://arxiv.org/abs/2303.11366
    - Agents verbally reflect on failures, maintain episodic memory. Our changelog and evolution state serve a similar function at the system level.

19. **Gou, Z., et al. (2024).** "CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing." *ICLR 2024*. https://arxiv.org/abs/2305.11738
    - LLMs verify claims using external tools. Related to our outer-review skill which uses different AI systems for cross-validation.

20. **Du, Y., Li, S., et al. (2024).** "Improving Factuality and Reasoning in Language Models through Multiagent Debate." *ICML 2024*. https://arxiv.org/abs/2305.14325
    - Multiple LLM instances debate to improve accuracy. Our pessimistic/optimistic review layers function as a structured form of adversarial debate.

21. **Qu, Y., et al. (2024).** "Recursive Introspection: Teaching Language Model Agents How to Self-Improve" (RISE). *NeurIPS 2024*. https://arxiv.org/abs/2407.18219
    - Fine-tuning for iterative self-correction. 17.7% improvement over 5 turns. Relevant to our deep-review cycles.

22. **Yao, S., et al. (2023).** "Tree of Thoughts: Deliberate Problem Solving with Large Language Models." *NeurIPS 2023*. https://arxiv.org/abs/2305.10601
    - Tree-structured reasoning with self-evaluation and backtracking. Foundational for self-evaluating reasoning systems.

29. **Estornell, A. & Liu, Y. (2024).** "Multi-LLM Debate: Framework, Principals, and Interventions." *NeurIPS 2024*. https://proceedings.neurips.cc/paper_files/paper/2024/hash/32e07a110c6c6acf1afbf2bf82b614ad-Abstract-Conference.html
    - Formalises multi-LLM debate mathematically; shows similar models converge to majority opinion (including shared misconceptions). Proposes three interventions that improve debate across BoolQ, MMLU, MathQ, TruthfulQA. Extends Du et al. (#20) with a more rigorous framework. Relevant to our use of different AI systems in outer review to avoid shared blind spots.

30. **Yang, R. & Narasimhan, K. (2023).** "The Socratic Method for Self-Discovery in Large Language Models." *Princeton NLP Group technical report*. https://princeton-nlp.github.io/SocraticAI/
    - Multi-agent Socratic dialogue with role-assigned LLM agents (analyst, analyst, proofreader) solving problems collaboratively. Conceptual contrast with our system: SocraticAI uses dialogue between agents for per-problem reasoning; our review layers use structured adversarial roles (pessimistic, optimistic, deep) for persistent knowledge-base improvement.

32. **Turpin, M., Michael, J., Perez, E., & Bowman, S.R. (2023).** "Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting." *NeurIPS 2023*. https://arxiv.org/abs/2305.04388
    - Demonstrates that CoT explanations can be systematically unfaithful: when biasing features are added, models fail to report true influences and rationalise incorrect answers (up to 36% accuracy drops). Directly motivates our architecture: if self-generated reasoning text can increase trust while decreasing epistemic safety, then *external* adversarial review (pessimistic review, outer review using different AI systems) is necessary rather than relying on self-critique alone.

## Category 6: AI Capability Context

33. **Karpathy, A. (2025).** "2025 LLM Year in Review." *Blog post (December 2025)*. https://knotbin.xyz/blog/2025-llm-year-in-review
    - Identifies a "qualitative threshold of coherence" crossed by coding agents around December 2025, making sustained autonomous workflows practical. Also coined "vibe coding" (February 2025) for one-shot AI-assisted development. Our work extends this to sustained, constrained, self-reviewing philosophical content production — what we call *agentic philosophy*.

## Category 7: Writing for AI Consumption

23. **Aggarwal, P., et al. (2024).** "GEO: Generative Engine Optimization." *KDD 2024*. https://arxiv.org/abs/2311.09735
    - **Key paper.** First academic work defining optimization for AI-generated search responses. Citations and statistics boost visibility by 40%. Our LLM-first content strategy is a related but distinct approach — we optimize for chatbot *consumption* rather than search *visibility*.

24. **Gao, Y., et al. (2024).** "Retrieval-Augmented Generation for Large Language Models: A Survey." *arXiv*. https://arxiv.org/abs/2312.10997
    - Comprehensive RAG survey covering how content structuring affects LLM retrieval. Background for our information architecture decisions.

## Gap Analysis

**What exists:**
- Single-shot AI article generation (STORM)
- AI mimicking a philosopher (Schwitzgebel)
- Self-refinement of individual responses (Self-Refine, Reflexion)
- Constitutional constraints for safety (Bai et al.)
- Content optimization for AI search engines (GEO)

**What doesn't exist (our contribution):**
- A system that **continuously evolves** a philosophical knowledge base through adversarial self-review
- **Tenet constraints applied to knowledge production** (not safety alignment)
- **Persistent, multi-layer review architecture** operating over months at corpus scale
- **Transparent human-AI co-authorship tracking** with contribution scoring
- **Content designed primarily for LLM consumption** in the philosophical domain
- **Convergence architecture** with section caps, coalescing, and synthesis layers

This gap is the paper's core contribution claim.
