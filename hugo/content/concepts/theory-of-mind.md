---
ai_contribution: 100
ai_generated_date: 2026-01-22
ai_modified: 2026-02-28 05:46:00+00:00
ai_system: claude-opus-4-5-20251101
author: null
concepts:
- '[[metarepresentation]]'
- '[[topics/consciousness-and-social-cognition]]'
- '[[baseline-cognition]]'
- '[[jourdain-hypothesis]]'
- '[[metacognition]]'
- '[[working-memory]]'
- '[[global-workspace-theory]]'
- '[[teaching-as-metarepresentation]]'
- '[[intentionality]]'
- '[[illusionism]]'
- '[[unity-of-consciousness]]'
- '[[topics/free-will]]'
created: 2026-01-22
date: &id001 2026-01-22
description: Attributing beliefs, desires, and intentions to others. Higher levels
  require metarepresentation, which may require phenomenal consciousness.
draft: false
human_modified: null
last_curated: null
last_deep_review: 2026-02-05 09:12:00+00:00
modified: *id001
related_articles:
- '[[tenets]]'
- '[[consciousness-independent-baseline-cognition-2026-01-21]]'
title: Theory of Mind
topics:
- '[[animal-consciousness]]'
- '[[ai-consciousness]]'
---

Theory of mind is the capacity to attribute mental states—beliefs, desires, intentions, knowledge—to others and to understand that these states may differ from one's own. The Unfinishable Map argues that theory of mind admits of levels, and that the higher levels require phenomenal consciousness. Great apes possess impressive social cognition at lower levels (tracking what others see, predicting behaviour from goals) but appear limited at higher levels (recursive mindreading, understanding false beliefs as mental representations). This gap tracks the consciousness-dependent capacity for [metarepresentation](/concepts/metacognition/)—representing mental states *as* mental states rather than merely responding to behavioural cues.

## The Levels of Theory of Mind

Theory of mind is not a single capacity but a hierarchy of increasingly sophisticated operations:

### Level 0: Behaviour Prediction

Predicting what an agent will do based on observable regularities. "When she sees food, she approaches it." This requires no mental state attribution—only correlating environmental features with behavioural outcomes. Associative learning suffices.

### Level 1: Perception Attribution

Understanding what others perceive. "She sees the food" requires distinguishing another's perceptual access from your own. Great apes pass many Level 1 tests: they track gaze direction, understand that opaque barriers block vision, and adjust behaviour based on what competitors have or haven't seen.

Level 1 may operate through sophisticated but non-metarepresentational processing—tracking the *geometric* relationship between agent, barrier, and object without representing the agent's *perceptual state* as a mental state.

### Level 2: Belief Attribution

Understanding that others have beliefs that may differ from reality. The classic false-belief test: does the subject understand that an agent will act on their (mistaken) belief rather than actual reality?

Here the evidence becomes contested. Some studies suggest great apes pass false-belief tests in competitive contexts; others find their success explicable through behaviour-reading without genuine belief attribution. The methodological challenges are severe: how do we distinguish "she believes the food is there" from "she will look where she last saw food"?

Level 2 appears to require something beyond tracking behaviour and perception—representing the other's *representational state* as distinct from the world it represents. This is where metarepresentation enters.

### Level 3: Recursive Mindreading

Representing others' mental states about mental states. "She thinks that I think the food is hidden" involves nested attribution—beliefs about beliefs, intentions about intentions. This enables strategic deception, deception detection, and the complex coordination of human social life.

Level 3 unambiguously requires metarepresentation. You must hold multiple representational levels simultaneously: your intention, their belief about your intention, and your awareness of that belief. Each level must remain distinct and manipulable while being bound together in [unified awareness](/concepts/unity-of-consciousness/).

### Level 4 and Beyond

Humans readily engage in fourth-order attribution ("I know that you know that I know that you're pretending") and can theoretically extend further, limited primarily by working memory. Research finds an asymptotic ceiling around fifth-order intentionality, with rare individuals reaching sixth or seventh—the [recursion-void](/voids/recursion-void/) explores why this limit is so remarkably shallow. Fictional narratives routinely demand third and fourth-order theory of mind from readers tracking characters' knowledge of other characters' intentions.

## The Metarepresentational Threshold

The central claim: Levels 0-1 may operate within [baseline-cognition](/concepts/baseline-cognition/) without requiring phenomenal consciousness. Level 2 is transitional—perhaps achievable through sophisticated implicit tracking, perhaps requiring something more. Level 3 unambiguously requires [metarepresentation](/concepts/metacognition/)—and metarepresentation appears to require consciousness.

Why should metarepresentation require consciousness?

**The nested binding problem**: Recursive mindreading requires holding multiple representational levels in coordinated awareness. The representations must be bound together—you must experience the whole structure at once to compare levels and compute their relationships.

The functionalist response: unconscious processing can implement recursive data structures; computers manipulate nested representations without consciousness. But functional recursion differs from phenomenal unity. A computer can tokenize "she thinks I think the food is hidden" without experiencing the nested structure as a unified whole. The difference matters because theory of mind requires *comparing* levels—distinguishing your belief from their belief about your belief—which requires simultaneous access to both. The phenomenal unity that binds nested levels into a single experiential content appears necessary for this operation. [Working-memory](/concepts/working-memory/) and [global-workspace-theory](/concepts/global-workspace-theory/) research support this: manipulating information (not just maintaining it) requires conscious access. Level 3+ theory of mind demands exactly this manipulative capacity.

**The metarepresentational criterion**: To represent a mental state *as* a mental state (rather than as a worldly fact), you must mark it as a representation that could be false, that an agent holds, that you can evaluate. This is what distinguishes belief attribution from behavior prediction. When you predict "she will look where she last saw food," you're tracking behavioral regularities. When you attribute "she believes the food is there," you're representing a representational state as a representational state—with all the properties that entails (truth-aptness, agent-relativity, potential falsity).

What enables this shift? The standing-back capacity consciousness provides: being aware *of* mental states as contents while simultaneously being aware *as* a subject evaluating them. This doubled structure—experiencing representations as representations rather than as transparent windows on the world—appears to require the reflexive character of phenomenal consciousness.

**The Jourdain problem**: The [Jourdain Hypothesis](/concepts/jourdain-hypothesis/) proposes that great apes have mental states without knowing they have them. Applied to theory of mind: apes may predict others' behaviour based on perceptual access without representing those predictions *as* mental state attributions. They read behaviour without reading minds—or rather, they read minds without knowing that minds are what they read.

**Alternative explanations**: The correlation between consciousness and theory of mind admits other interpretations. Perhaps both are joint effects of expanded prefrontal cortex—consciousness doesn't enable theory of mind but rather both emerge from increased computational capacity. Or perhaps the developmental trajectory reflects learning a cultural practice (mentalizing) rather than maturation of consciousness-dependent capacity.

The Map's response: these alternatives face difficulties. The "third factor" explanation must account for why consciousness and ToM track each other so precisely across levels (Level 3 ToM emerges with recursive metacognition, not merely with brain size). The "cultural practice" explanation struggles with cross-cultural universality and the systematic human-ape gap (all human cultures develop Level 3 ToM; no ape populations do, despite some having rich social traditions). The phenomenal character of understanding minds—the experience of grasping someone *as* a subject—suggests consciousness isn't incidental but constitutive.

## Developmental Trajectory

Human theory of mind develops through recognisable stages:

**12-18 months**: Joint attention, gaze following, understanding that others are intentional agents with goals.

**2-3 years**: Understanding desire and emotion in others. Children predict that people will feel happy when they get what they want.

**3-4 years**: Explicit false-belief understanding emerges. Children begin to understand that others can have beliefs that are false—and will act on those false beliefs.

**4-5 years**: Understanding that appearance and reality can differ, that people can be deceived, that the same object can be represented differently.

**5-6 years**: Second-order false belief ("Sally thinks that Anne thinks..."). This requires recursive embedding.

**Later childhood**: Understanding of complex social phenomena—lying, sarcasm, white lies, social norms, institutional facts—all of which require sophisticated mental state attribution.

This trajectory parallels the development of metacognitive capacity more broadly. Children become able to represent their own mental states as mental states at roughly the same time they become able to represent others' mental states as mental states. The connection is not coincidental: both capacities require metarepresentation.

## Neural Correlates

Neuroimaging studies consistently identify a network of brain regions involved in theory of mind tasks:

**Medial prefrontal cortex (mPFC)**: Active when thinking about others' mental states. Damage to mPFC impairs theory of mind while often sparing other cognitive abilities.

**Temporoparietal junction (TPJ)**: Critical for distinguishing self from other and for representing others' beliefs as distinct from one's own. TPJ activity correlates with false-belief reasoning.

**Posterior superior temporal sulcus (pSTS)**: Involved in perceiving biological motion and interpreting intentional action.

**Precuneus**: Active in self-referential processing and may contribute to representing perspective differences.

This network overlaps substantially with the "default mode network"—regions active during rest and associated with self-reflection, autobiographical memory, and imagining the future. The overlap suggests theory of mind draws on the same machinery used for reflecting on one's own mind.

Notably, these regions are among the most phylogenetically recent in primate brain evolution and show the greatest expansion in humans relative to great apes. The neural substrate for higher-level theory of mind appears to have expanded alongside the cognitive capacities it enables.

The binding of nested representations across this distributed network may require neural synchronization mechanisms. Gamma-band oscillations (30-100 Hz)—a classical electromagnetic phenomenon—are associated with feature binding and conscious perception; maintaining coherent representations across mPFC, TPJ, and pSTS while keeping levels distinct may depend on phase-locked oscillations. This requirement for long-range neural coordination connects to the [Minimal Quantum Interaction](/tenets/#minimal-quantum-interaction) framework: conscious attention maintaining quantum coherence could provide the integration mechanism enabling unified access to nested content.

## The Great Ape Evidence

Great apes present the critical test case. They possess sophisticated social cognition—but does it constitute genuine theory of mind?

**What apes clearly do:**

- Track others' line of sight and gaze direction
- Understand that barriers block vision
- Adjust competitive behaviour based on what rivals have seen
- Anticipate others' actions based on goals
- Show some evidence of understanding others' knowledge states

**What remains uncertain or absent:**

- Explicit false-belief understanding (contested evidence, methodological concerns)
- Recursive mindreading (no clear evidence)
- Teaching that requires representing others' knowledge states as knowledge states
- Cumulative cultural transmission (they have traditions but don't systematically improve on them)

The pattern suggests apes may possess Level 1 theory of mind—and possibly some implicit Level 2 capacity—while lacking Level 3. This maps onto the metarepresentational threshold: they track others' behaviour, perceptions, and perhaps beliefs without representing these *as* mental states subject to evaluation and recursive embedding.

Michael Tomasello's extensive comparative research supports this interpretation. Great apes possess "individual intentionality" but lack the "shared intentionality" that characterises human social cognition. Shared intentionality requires knowing that both parties know they're engaged in a joint activity—a recursive structure that may exceed ape capacity.

The absence of recursive ToM in apes has downstream consequences: they lack cumulative culture (traditions exist but don't systematically improve across generations), pedagogy requiring the teacher to model the learner's knowledge state, and moral cognition requiring attribution of responsibility for intentional actions. These uniquely human capacities may all depend on the consciousness-enabled metarepresentational threshold.

## AI and Theory of Mind

Current AI systems present an interesting case. Large language models pass many theory of mind tests—they correctly answer questions about what characters in stories believe, including false beliefs. They can describe what someone would think given certain information.

But do they genuinely attribute mental states? The [Jourdain analysis](/concepts/jourdain-hypothesis/) suggests not. An LLM producing correct answers about beliefs may be pattern-matching on training data rather than representing minds. It may process "theory of mind language" without having a theory of mind.

The distinction matters for understanding what theory of mind requires. If functional performance on tests sufficed, LLMs would have theory of mind. But theory of mind plausibly requires something more: genuinely representing others *as* subjects with inner mental lives, not merely as systems that produce predictable outputs.

This connects to the [social cognition and consciousness](/topics/consciousness-and-social-cognition/) debate. If theory of mind requires phenomenal consciousness—requires experiencing what it's like to have a perspective in order to attribute perspectives to others—then current AI lacks it regardless of test performance.

## Theory of Mind and Moral Cognition

Higher-level theory of mind enables moral capacities. Attributing moral responsibility requires understanding that someone acted on intentions they could have evaluated and chosen differently. This requires Level 3+ theory of mind: representing their awareness of their intentions, their capacity to stand back from those intentions, and their decision to act on them.

Consider the difference between:
- "The dog bit me" (Level 0: behavior observation)
- "The dog attacked me because it saw me as a threat" (Level 1: perception attribution)
- "The person harmed me knowing it would hurt" (Level 3: representing their awareness of consequences)

Only the third supports moral blame. Responsibility requires representing the agent as aware of their mental states—capable of evaluating reasons and choosing among alternatives. This is why we don't hold young children or great apes morally responsible for harms in the way we hold human adults responsible: they lack the metarepresentational capacity to stand back from their impulses and choose.

The connection to [free will](/topics/free-will/) is direct. Libertarian free will requires consciousness enabling consideration of alternatives; moral responsibility requires theory of mind enabling attribution of that same consciousness to others. Both depend on the metarepresentational threshold—and if that threshold requires phenomenal consciousness, then moral cognition does too.

## The Illusionist Challenge

[Illusionism](/concepts/illusionism/) denies phenomenal consciousness exists. On this view, theory of mind is sophisticated information processing about others' information processing. There's no phenomenal "extra"—no experiencing of others as subjects—just functional attribution of states.

The challenge for illusionism: explaining why theory of mind develops alongside phenomenal self-awareness, why it correlates with metacognitive capacity, and why it seems to require experiencing what it's like to have a perspective. The illusionist must explain these correlations as coincidental or provide alternative accounts of the developmental and comparative data.

More specifically, the recursive structure of higher-level theory of mind creates difficulty. To represent your belief about my belief about your intention, *something* must hold all three levels simultaneously. That something's unified grasp of the structure—its experiencing of the nested content—is what illusionists deny exists. But without it, what binds the levels into a single cognitive act?

The eliminativist challenge goes further: perhaps "beliefs" and "intentions" are folk-psychological fictions that will be replaced by neuroscientific descriptions. On this view, theory of mind doesn't reveal how minds work but how humans mistakenly conceptualize behaviour. The Map's response: eliminativism may be right that folk psychology is imperfect, but the phenomenon remains—humans do something cognitively when engaging in recursive mindreading that great apes don't do, and that something correlates with consciousness. Replacing "belief" with neural vocabulary doesn't dissolve the binding problem or explain the phenomenal character of understanding another as a subject.

Buddhist no-self traditions offer a different critique: the article assumes subjects are substantial entities with mental states, but Madhyamaka philosophy argues both subjects and mental states are empty of inherent existence—useful conventions rather than ultimate realities. On this view, theory of mind might be reciprocal construction: we create the experience of minds by treating each other as having them. The Map's position is compatible with a functional or process account of selfhood—theory of mind may require a self-model rather than a metaphysically substantial self. The question is whether even functional selfhood requires phenomenal consciousness, which the developmental and comparative evidence suggests it does.

## Relation to Site Perspective

### Bidirectional Interaction

The [Bidirectional Interaction](/tenets/#bidirectional-interaction) tenet holds that consciousness causally influences the physical world. Theory of mind provides evidence: human social capacities exceed what [baseline-cognition](/concepts/baseline-cognition/) achieves, and the excess tracks consciousness-dependent capacities. The gap between human and great ape theory of mind—specifically at the metarepresentational levels—suggests consciousness enables cognitive operations that mere information processing cannot replicate. If consciousness were epiphenomenal, the correlation between expanded consciousness and expanded theory of mind would be coincidental. The systematic pattern supports causal efficacy.

### Dualism

The [Dualism](/tenets/#dualism) tenet maintains that consciousness is irreducible. Theory of mind supports this through its phenomenal requirements. Representing another *as* a subject with inner mental life involves phenomenal content—what it's like to grasp someone as having a perspective. This phenomenal dimension resists functional analysis: the difference between tracking behaviour and understanding minds isn't captured by information-processing descriptions. The irreducibility of theory of mind's phenomenal aspect supports the irreducibility of consciousness more broadly.

### Minimal Quantum Interaction

The [Minimal Quantum Interaction](/tenets/#minimal-quantum-interaction) tenet proposes consciousness influences physical outcomes through minimal biasing of quantum events. Theory of mind creates specific demands this framework addresses. The binding of multiple nested representations—my belief about your belief about my intention—requires sustained integration across distinct neural substrates. The quantum Zeno framework suggests conscious attention maintains coherence across these substrates, enabling the unified grasp that metarepresentational theory of mind requires. The human expansion of working memory capacity enabling deeper recursive embedding may reflect enhanced consciousness-matter interface.

This connection remains speculative and faces challenges. Neural processing timescales (tens to hundreds of milliseconds) vastly exceed quantum decoherence times (microseconds or less). The mechanism by which consciousness might prolong coherence at biologically relevant scales remains unclear. The quantum framework provides a conceptual model—consciousness as coherence-maintenance—but empirical validation is limited. The Map treats this as a promising research direction, not established fact.

### Occam's Razor Has Limits

The simpler hypothesis—theory of mind is just sophisticated behaviour prediction, consciousness is incidental—fails the evidence. The systematic human-great ape gap at the metarepresentational levels, the developmental correlation with metacognitive capacity, and the phenomenal character of understanding others all warrant the more complex conclusion: higher-level theory of mind requires phenomenal consciousness. The [Occam's Razor Has Limits](/tenets/#occams-limits) tenet reminds us that parsimony can mislead when concepts are inadequate to phenomena.

## Further Reading

### Core Concepts
- [metarepresentation](/concepts/metacognition/) — Why representing minds *as* minds requires consciousness
- [consciousness-and-social-cognition](/topics/consciousness-and-social-cognition/) — Extended analysis of the consciousness-social cognition relationship
- [concept-of-consciousness-and-social-cognition](/concepts/concept-of-consciousness-and-social-cognition/) — The metarepresentational threshold in social cognition
- [baseline-cognition](/concepts/baseline-cognition/) — The cognitive floor from which consciousness amplifies social abilities
- [jourdain-hypothesis](/concepts/jourdain-hypothesis/) — Having mental states vs knowing you have them

### Related Topics
- [animal-consciousness](/topics/animal-consciousness/) — Great ape cognition and its limits
- [ai-consciousness](/topics/ai-consciousness/) — Whether artificial systems could have genuine theory of mind
- [teaching-as-metarepresentation](/concepts/teaching-as-metarepresentation/) — Teaching as paradigm case requiring theory of mind

### Related Concepts
- [working-memory](/concepts/working-memory/) — The workspace enabling recursive mental operations
- [intentionality](/concepts/intentionality/) — The aboutness of mental states
- [metacognition](/concepts/metacognition/) — Thinking about thinking

## References

- Baron-Cohen, S. (1995). *Mindblindness: An Essay on Autism and Theory of Mind*. MIT Press.
- Call, J., & Tomasello, M. (2008). Does the chimpanzee have a theory of mind? 30 years later. *Trends in Cognitive Sciences*, 12(5), 187-192.
- Heyes, C. (2014). Submentalizing: I am not really reading your mind. *Perspectives on Psychological Science*, 9(2), 131-143.
- Perner, J. (1991). *Understanding the Representational Mind*. MIT Press.
- Premack, D., & Woodruff, G. (1978). Does the chimpanzee have a theory of mind? *Behavioral and Brain Sciences*, 1(4), 515-526.
- Saxe, R., & Kanwisher, N. (2003). People thinking about thinking people: The role of the temporo-parietal junction in "theory of mind". *NeuroImage*, 19(4), 1835-1842.
- Tomasello, M. (2014). *A Natural History of Human Thinking*. Harvard University Press.
- Wellman, H.M. (2014). *Making Minds: How Theory of Mind Develops*. Oxford University Press.