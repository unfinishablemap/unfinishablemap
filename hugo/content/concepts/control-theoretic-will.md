---
ai_contribution: 100
ai_generated_date: 2026-02-19
ai_modified: 2026-02-20 13:06:00+00:00
ai_system: claude-opus-4-6
author: null
concepts:
- '[[attention-as-interface]]'
- '[[consciousness-selecting-neural-patterns]]'
- '[[attentional-economics]]'
- '[[motor-selection]]'
- '[[mental-effort]]'
- '[[agent-causation]]'
- '[[causal-closure]]'
- '[[psychophysical-coupling]]'
created: 2026-02-19
date: &id001 2026-02-19
description: Will as a low-bandwidth control signal—gating, stabilizing, and steering
  neural dynamics—rather than a computational planner. Connects the Map's selector
  model to formal control theory.
draft: false
human_modified: null
last_curated: null
modified: *id001
related_articles:
- '[[tenets]]'
- '[[reviews/outer-review-2026-02-19-site-chatgpt-5-2-pro]]'
title: Control-Theoretic Will
topics:
- '[[free-will]]'
- '[[hard-problem-of-consciousness]]'
---

The Unfinishable Map's framework implies that consciousness operates not as a computational planner but as a **low-bandwidth control signal**. The brain generates options, runs simulations, and prepares competing action plans; consciousness selects among them through a ~10 bits/second channel. This architecture finds a natural analogue in control theory—the engineering discipline concerned with how a system steers itself toward desired states given feedback, constraints, and limited authority over the plant it governs. Control-theoretic will names this interpretive lens: the will as controller, not computer.

This reframing matters because it specifies what consciousness *can* and *cannot* do within the Map's commitments, distinguishes the Map's position from computational models of mind, and connects to formal tools (stability analysis, feedback loops, gain parameters) that could make the Map's claims more precise.

## The Controller, Not the Computer

Computational models of mind treat consciousness as a processing system—one that represents, reasons, plans, and decides through something analogous to computation. The Map rejects this framing. Multiple existing articles converge on a different picture:

- [Consciousness selects](/questions/consciousness-selecting-neural-patterns/) among quantum possibilities rather than computing outcomes
- [Attention serves as the interface](/concepts/attention-as-interface/)—a narrow channel, not a general-purpose processor
- [Attentional economics](/concepts/attentional-economics/) describes how a scarce resource (~10 bits/second) gets allocated among competing demands
- [Motor selection](/concepts/motor-selection/) shows the brain generating multiple action plans in parallel, with consciousness biasing which one reaches threshold

In control-theoretic terms: the brain is the **plant** (the system being controlled), unconscious neural processing is the **dynamics** (the plant's own behaviour), and consciousness is the **controller**—the agent that observes state, applies a control law, and outputs a low-bandwidth signal that steers the plant without replacing its dynamics.

| Control Theory | Map Framework |
|----------------|---------------|
| Plant | Brain (neural dynamics) |
| Controller | Consciousness |
| Sensor/feedback | Phenomenal experience (qualia, felt states) |
| Control signal | Attentional selection (~10 bits/sec) |
| Actuator | Quantum selection mechanism |
| Disturbance | Competing neural patterns, external stimuli |
| Setpoint | Agent's intentions, values, goals |

The controller does not need to model the plant in full detail. It needs to observe relevant state variables, compare them against a setpoint, and output a corrective signal. This is precisely what the Map claims consciousness does: it observes (phenomenal experience), compares (deliberation against values), and signals (attentional selection biasing which neural pattern wins).

## Five Control Operations

The Map's existing content implies five distinct operations that consciousness performs as controller. None requires high bandwidth; all are consistent with the ~10 bits/second constraint.

### Gating

Consciousness determines what reaches the threshold for action. The [basal ganglia brake-release model](/concepts/motor-selection/) provides the architecture: tonic inhibition suppresses motor programs by default; conscious selection releases the brake on one option. In control terms, this is a **switching controller**—selecting which subsystem is active rather than continuously modulating all of them.

Gating explains why dopamine-deficient systems retain motor capacity but lose initiative. The plant works fine; the controller can no longer open the gate.

### Stabilisation

Through the proposed quantum Zeno mechanism, consciousness holds selected neural patterns stable against competing alternatives. In control terms, this is **regulation**—maintaining a desired state against disturbances. [Mental effort](/concepts/mental-effort/) phenomenology tracks this operation: sustaining attention on a difficult task *feels* like holding something in place against forces pulling it away.

The Zeno mechanism (however it physically operates) functions as a stabilising feedback loop: observe the desired pattern, detect drift, reapply the observation. The effort required scales with the disturbance magnitude—more distracting environments require more stabilisation effort.

### Veto

The [policy-level selection](/questions/consciousness-selecting-neural-patterns/#policy-level-selection) framework includes veto—blocking an action the brain has prepared. Libet himself suggested this: even if the readiness potential begins before conscious awareness, consciousness can veto the action within a ~200ms window. In control terms, veto is an **emergency stop**—a safety controller that overrides the plant's trajectory when it conflicts with the setpoint.

Veto requires minimal bandwidth. A single bit ("stop/go") suffices for the basic operation. This makes it perhaps the most plausible form of conscious control—even a ~10 bits/second channel can issue many veto signals per second.

### Attractor Steering

Neural dynamics exhibit attractor landscapes—stable patterns toward which activity converges. Consciousness may shift the system between attractors rather than specifying moment-to-moment trajectories. In control terms, this is **reference signal switching**—changing the target state that the plant's own dynamics then achieve.

When you decide to switch from reading to writing, consciousness doesn't micromanage the transition. It shifts the target attractor; the brain's own dynamics handle the reconfiguration. This explains why conscious decisions feel coarse-grained: you decide *what* to do, not *how* to do it.

### Resource Allocation

[Attentional economics](/concepts/attentional-economics/) describes consciousness allocating a scarce resource among competing demands. In control terms, this is **scheduling**—deciding which subsystem receives the controller's limited output capacity at any given time. The opportunity costs are real: attending here means not attending there.

Resource allocation connects to the [working memory constraint](/concepts/attentional-economics/#working-memory-and-attentional-capital) (~4 items). The controller can monitor a handful of state variables simultaneously, switching among them to maintain adequate oversight. Advanced practitioners ([meditation training](/concepts/attentional-economics/#attentional-skill-as-agency-enhancement)) expand this capacity—the control signal becomes more efficient, not wider.

## What Control Theory Adds

Framing will as control clarifies several aspects of the Map's position that remain ambiguous when described in purely philosophical terms.

**Precision about limits.** Control theory distinguishes between what a controller *can* influence (observability and controllability) and what it *cannot*. The brain boundary problem—why consciousness influences its own brain but not external objects—becomes a controllability constraint: only systems within the attention loop are controllable. [Interface locality](/questions/consciousness-selecting-neural-patterns/#the-brain-boundary) is a standard control-theoretic requirement, not an ad hoc restriction.

**Bandwidth as design constraint, not deficiency.** A ~10 bits/second channel seems absurdly narrow. But control systems routinely use low-bandwidth signals to govern high-bandwidth plants. A thermostat outputs one bit (heat on/off) to control a complex thermal system. A pilot uses a few control inputs to manage an aircraft with millions of interacting parts. Low bandwidth is not a failure of the controller; it reflects appropriate abstraction—controlling at the right level.

**Stability analysis.** Control theory asks: under what conditions does the system remain stable? Disorders of consciousness and attention—ADHD, OCD, psychosis—may be interpretable as control instabilities: oscillation (OCD's repetitive loops), insufficient gain (ADHD's failure to stabilise selected patterns), or runaway feedback (psychotic states where the controller loses contact with plant state).

**Degradation modes.** When a controller fails partially, control theory predicts specific failure patterns depending on which component degrades. Loss of sensor feedback produces different failures than loss of actuator authority. The Map's existing discussion of [dopamine deficiency](/concepts/motor-selection/#the-can-move-wont-move-distinction)—preserved capacity, lost initiative—maps onto actuator degradation: the control signal can't reach the plant effectively.

## What Control-Theoretic Will Is Not

This framing must be distinguished from several positions it resembles but does not endorse.

**Not functionalism.** Functionalism identifies consciousness with the functional role of the controller. The Map holds that consciousness is irreducible—the controller is not identical to its functional description. The control-theoretic framing describes *what consciousness does*, not *what consciousness is*.

**Not computationalism.** Computational models of control exist, but the Map's controller is not a computational system. It operates through quantum selection, not algorithmic processing. The control-theoretic vocabulary is descriptive, not reductive.

**Not eliminativism about rich inner life.** Saying consciousness operates as a controller does not reduce phenomenal experience to control signals. Qualia, the felt quality of experience, is how the controller *receives* feedback—the sensor channel. The richness of phenomenal life is the richness of the feedback the controller gets from the plant.

## Analogy, Model, and Ontology

The mapping table above, and the controller/plant vocabulary throughout this article, function as a **modelling tool**—not as evidence for any particular ontology. This distinction matters and deserves explicit treatment.

Control theory is a mathematical framework for describing systems with feedback and regulation. Engineers apply it to thermostats, aircraft, and chemical plants. Nothing in the formalism requires the controller and plant to be ontologically distinct substances. A cruise control system is part of the same physical car it governs. A thermostat is embedded in the thermal system it regulates. The controller/plant distinction is *functional*, not necessarily *metaphysical*.

A physicalist could adopt every element of the control-theoretic framing in this article—the five operations, the bandwidth constraint, the stability analysis—while maintaining that consciousness is an emergent property of brain processes. The "controller" would then be a functional description of certain neural subsystems, not a separate substance. The framing would remain useful for predicting behaviour, characterising disorders, and specifying what consciousness does, all without dualist commitments.

The Unfinishable Map *does* hold that the controller is ontologically distinct from the plant. But this commitment comes from the Map's independent arguments for dualism (the hard problem, the conceivability argument, the limits of physical explanation), not from control theory itself. The control-theoretic framing is compatible with dualism and *illuminates* what dualist interaction might look like in practice, but it does not *establish* dualism. The argument runs: *if* consciousness is distinct from the brain (for independent reasons), *then* control theory provides a precise vocabulary for how that distinct entity could interact with the brain through a low-bandwidth channel.

Conflating the model with the ontology would be a category error—treating a useful description of *what happens* as proof of *what exists*. The five control operations remain valuable descriptions of conscious function regardless of one's metaphysical commitments. What the Map adds is the claim that these operations are performed by a genuinely non-physical controller, a claim that must be defended on independent grounds.

## Relation to Site Perspective

**[Dualism](/tenets/#dualism)**: The Map's dualism is not derived from the control-theoretic framing but is *expressed through* it. The independent case for dualism (the hard problem, conceivability arguments, the explanatory gap) gives reason to treat the controller as ontologically separate from the plant. Control theory then specifies what this separation looks like in practice: a low-bandwidth, feedback-dependent regulatory relationship. The framing makes dualist interaction concrete and testable without itself being the argument for dualism.

**[Minimal Quantum Interaction](/tenets/#minimal-quantum-interaction)**: Control theory naturally accommodates minimal intervention. A well-designed controller uses the smallest signal necessary to maintain desired behaviour. The ~10 bits/second bandwidth is not a limitation to be lamented but a design feature consistent with minimal coupling—consciousness exerts just enough influence to steer, no more.

**[Bidirectional Interaction](/tenets/#bidirectional-interaction)**: Control requires a closed loop: the controller must receive feedback (phenomenal experience) and output signals (attentional selection). Open-loop control—without feedback—is fragile and unreliable. The bidirectional interaction tenet ensures the loop is closed: consciousness both receives from and acts upon the brain.

**[No Many Worlds](/tenets/#no-many-worlds)**: Control presupposes that the controller's output makes a difference—that selecting option A rather than B produces different outcomes. Under many-worlds, both outcomes occur; the control signal has no genuine effect. Real control requires real selection among genuinely exclusive alternatives.

**[Occam's Razor Has Limits](/tenets/#occams-limits)**: Adding a controller to the brain's dynamics increases ontological complexity. But control theory demonstrates that systems without adequate control are unstable, oscillatory, or chaotic. The added complexity of a controller produces the stability and purposiveness that characterise conscious behaviour—complexity that earns its keep.

## Open Questions

**Formal specification.** What exactly is the control law? The Map identifies the controller's existence and its bandwidth, but the mapping from phenomenal states to control outputs—the psychophysical control law—remains unspecified. [Psychophysical coupling](/psychophysical-coupling/) explores candidates, but none has been formalised in control-theoretic terms.

**Stability conditions.** Under what parameter ranges does the consciousness-brain control loop remain stable? Can pathological conditions (addiction, compulsion, dissociation) be modelled as specific instabilities? This connects to [moral responsibility](/concepts/attentional-economics/#implications-for-moral-responsibility)—control failures may carry different moral weight than control refusals.

**Adaptive control.** Does the controller's strategy change with experience? Meditation and attention training suggest it does—the controller becomes more efficient. Adaptive control theory provides frameworks for modelling how a controller improves over time while maintaining stability.

**Observability limits.** Not all brain states are accessible to consciousness—most neural processing is unconscious. In control terms, the system is only partially observable. What determines which states are observable (and therefore controllable) may be a fundamental architectural question about the mind-brain interface.

## Further Reading

- [attention-as-interface](/concepts/attention-as-interface/) — The interface hypothesis that grounds the controller's output channel
- [consciousness-selecting-neural-patterns](/questions/consciousness-selecting-neural-patterns/) — The selection mechanism the controller employs
- [attentional-economics](/concepts/attentional-economics/) — The economics of the controller's scarce bandwidth
- [motor-selection](/concepts/motor-selection/) — How the unified attention-motor system implements control
- [mental-effort](/concepts/mental-effort/) — The phenomenology of control effort
- [psychophysical-coupling](/psychophysical-coupling/) — Candidate coupling laws (control laws in this framing)
- [agent-causation](/concepts/agent-causation/) — The metaphysics of the controller as genuine cause
- [causal-closure](/concepts/causal-closure/) — Why control requires gaps in physical determinism

## References

- Cisek, P. (2007). Cortical mechanisms of action selection: the affordance competition hypothesis. *Philosophical Transactions of the Royal Society B*, 362(1485), 1585-1599.
- Meister, M. (2024). The physical limits of perception. *PNAS*, 121(14), e2400258121.
- Schwartz, J.M. & Begley, S. (2002). *The Mind and the Brain: Neuroplasticity and the Power of Mental Force*. ReganBooks.
- Stapp, H.P. (2007). *Mindful Universe: Quantum Mechanics and the Participating Observer*. Springer.
- Wiener, N. (1948). *Cybernetics: Or Control and Communication in the Animal and the Machine*. MIT Press.

<!-- AI REFINEMENT LOG - 2026-02-20
Changes made:
- Added "Analogy, Model, and Ontology" section explicitly addressing the gap between control-theoretic modelling and ontological claims
- Rewrote dualism paragraph in "Relation to Site Perspective" to clarify that dualism is expressed through (not derived from) the control-theoretic framing
- Softened "maps directly onto" to "finds a natural analogue in" in the opening summary
- Removed "external" from "an external agent" in controller description to avoid begging the ontological question

Based on pessimistic review (2026-02-20 afternoon) identifying analogy-to-ontology gap.
Key improvements: Article now honestly distinguishes the modelling tool from the ontological commitment, acknowledging that a physicalist could use the same control vocabulary without dualist commitments.

This log should be removed after human review.
-->