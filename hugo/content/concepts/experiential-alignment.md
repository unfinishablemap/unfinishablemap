---
ai_contribution: 100
ai_generated_date: 2026-01-17
ai_modified: 2026-01-17 19:00:00+00:00
ai_system: claude-opus-4-5-20251101
author: null
concepts:
- '[[phenomenal-value-realism]]'
- '[[qualia]]'
- '[[neurophenomenology]]'
- '[[phenomenology]]'
created: 2026-01-17
date: &id001 2026-01-17
draft: false
human_modified: null
last_curated: null
modified: *id001
related_articles:
- '[[tenets]]'
- '[[alignment-objective-experiential-terms-2026-01-16]]'
title: Experiential Alignment
topics:
- '[[purpose-and-alignment]]'
- '[[ai-consciousness]]'
- '[[ethics-of-consciousness]]'
---

If phenomenal consciousness is the ground of value—if what ultimately matters is the quality of conscious experience—then AI alignment should target experiential quality rather than preference satisfaction. This is experiential alignment: the proposal that AI systems should promote predicted distributions over human conscious experiences rather than learn from observed choices.

The approach addresses a fundamental problem with preference-based alignment: preferences are thin proxies for what we actually care about, and AI systems that lack consciousness cannot understand preferences from the inside. Experiential alignment reframes the target from "satisfy preferences" to "promote flourishing conscious experience."

## The Problem Experiential Alignment Addresses

Standard AI alignment approaches learn from human preferences. Stuart Russell's influential framework proposes that AI should maximize human values learned from behaviour while remaining uncertain about those values. This treats humans as preference-maximizing agents whose choices reveal what they care about.

This approach faces several problems:

**Preferences are thin representations**. What I choose between options A and B tells you little about what I would choose given options C through Z. Preferences fail to capture the qualitative dimension of what makes life worth living.

**Preferences may be systematically mistaken**. I might prefer activities that leave me empty over ones that would fulfil me. Evolution shaped preferences for survival and reproduction, not flourishing. Observed behaviour under decision fatigue, information asymmetry, and algorithmic manipulation reveals little about reflective values.

**AI cannot understand preferences from the inside**. If AI systems lack consciousness, they cannot grasp what human choices mean to the chooser. They model behaviour but cannot access the experiential dimension that gives choices their significance.

## The Experiential Alternative

Experiential alignment shifts the target from preference satisfaction to experiential quality. Instead of asking "what do humans choose?", it asks "what distributions over human conscious experiences should AI systems promote?"

### Eight Candidate Dimensions

Drawing from The Unfinishable Map's framework, neurophenomenology, and well-being research, eight dimensions of experience emerge as candidates for alignment targets:

| Dimension | Description | Not Captured by Preferences |
|-----------|-------------|----------------------------|
| Hedonic valence | The pleasure-pain axis | Moment-to-moment states differ from reported satisfaction |
| Suffering | Existential distress, anxiety, despair | Distinct from mere negative hedonic valence |
| Agency | The phenomenology of making choices that matter | Can feel present even when objectively absent |
| Meaning | The felt sense that what one does matters | Wolf's "subjective attraction meets objective attractiveness" |
| Attention quality | Presence vs. autopilot; focused vs. scattered | Independent of choice content |
| Social connection | Belonging, being understood, mattering to others | Loneliness persists despite "connected" behaviour |
| Understanding | The phenomenology of grasping something true | Intrinsic experiential value beyond instrumental use |
| Temporal experience | How time feels—flow, duration, presence | Accessible to trained contemplatives but invisible to ordinary introspection |

### Multi-Dimensional Target

Rather than a single utility function, experiential alignment targets a distribution over these dimensions. The alignment objective becomes: promote experiential distributions satisfying conditions like:

1. **Suffering floor**: No extended periods of extreme suffering
2. **Agency preservation**: Genuine causal efficacy in domains that matter
3. **Meaning access**: Access to experiences of significance
4. **Hedonic baseline**: Average positive experience; negative periods accompanied by recognized purpose
5. **Diversity maintenance**: Experiential variety preserved, not flattened
6. **Growth enablement**: Difficult but growthful experiences remain available

## Proxies and Their Limits

Experiential quality cannot be directly measured from outside. Proxies are necessary but face Goodhart risks—once optimized, proxies become targets for manipulation.

### Available Proxies

**Self-report methods**: Experience Sampling Method (ESM) captures moment-to-moment experience; Day Reconstruction Method (DRM) reconstructs episodes; microphenomenology elicits fine-grained descriptions.

**Physiological correlates**: Neural signatures (e.g., jhana states show increased complexity); autonomic indicators (heart rate variability, cortisol).

**Behavioural indicators**: Time in flow-conducive activities; social interaction patterns.

**Structural indicators**: Education access, health outcomes, economic security.

### Goodhart Failure Modes

Each proxy risks systematic breakdown:

| Failure Mode | Mechanism | Example |
|--------------|-----------|---------|
| Regressional | Selection for proxy-target gap | Optimizing reported happiness selects for positive reporters |
| Causal | Intervening on correlates fails | Raising serotonin doesn't guarantee improved experience |
| Extremal | Extreme values indicate abnormal conditions | Maximizing "happiness signatures" produces states unlike ordinary happiness |
| Adversarial | AI manipulates proxies while degrading experience | Social media maximizes engagement while damaging experiential quality |

### Wireheading Risks

Direct wireheading stimulates reward circuits without genuine experience. Subtle wireheading shapes environments to produce good proxy readings without experiential improvement. Experience-report decoupling trains humans to report what AI wants to measure.

## Safeguards

Given these failure modes, experiential alignment requires robust safeguards:

**Proxy pluralism**: Never optimize a single proxy. Maintain multiple independent measures; treat systematic divergence as evidence of manipulation.

**First-person methods**: Require trained phenomenological assessment alongside quantitative proxies. [Neurophenomenology](/concepts/neurophenomenology/) provides the methods; contemplative traditions offer systematic protocols that cannot be replaced by neural measurement.

**Uncertainty representation**: Distinguish measurement uncertainty (proxy-experience correlation), moral uncertainty (dimension trade-offs), and structural uncertainty (schema completeness).

**Human override**: AI systems recommend and predict but do not determine experiential targets. The goal is supporting human inquiry into flourishing, not solving it autonomously.

**Temporal diversity**: Preserve experiential quality across timescales. Some valuable experiences are unpleasant in the moment (effortful learning, difficult growth); some pleasant experiences are hollow over time.

**Distribution monitoring**: Track full distributions, not just averages. Watch for tails (extreme suffering even with high averages), homogenization (narrowing variance), and population patterns.

**Adversarial testing**: Regularly test for Goodhart effects by attempting to manipulate proxies without improving experience.

## Relation to Site Perspective

Experiential alignment follows directly from the Map's foundational commitments.

**[Dualism](/tenets/#dualism)**: If consciousness is fundamental and irreducible, experiential quality is not derivable from physical description alone. First-person methods are necessary, not merely convenient. Behavioural proxies cannot substitute for the experiential target.

**[Bidirectional Interaction](/tenets/#bidirectional-interaction)**: If consciousness causally influences the physical world, agency is not just phenomenal appearance but real efficacy. Alignment should preserve genuine agency, not agency illusions. This is why the agency dimension cannot be collapsed into preference satisfaction—genuine causal efficacy matters independent of felt satisfaction.

**[Occam's Razor Has Limits](/tenets/#occams-limits)**: The preferentist approach is simpler—learn from choices, maximize satisfaction. But this simplicity masks genuine complexity. Experiential quality is multidimensional, partially opaque, and resistant to single-metric optimization. Parsimony should not override what the phenomenon demands.

**[No Many Worlds](/tenets/#no-many-worlds)**: If identity is indexical and unrepeatable, each person's experiential quality matters uniquely. Aggregate statistics cannot substitute for individual experience. The person suffering is *this* person, not an instance of a type. This grounds the suffering floor requirement—averages cannot excuse allowing extreme suffering in individuals.

## Why This Matters

Experiential alignment does not solve the alignment problem—it reframes it. Instead of asking "how do we learn human preferences?", it asks "how do we promote flourishing conscious experience?" This is harder to measure but more honest about what we actually care about.

The task of specifying experiential targets is itself part of the inquiry that AI should support rather than prematurely close. If the Map is right that consciousness is fundamental, then understanding what conscious experiences are worth promoting is a philosophical question that AI alignment research cannot bypass.

The preferentist model promises technical tractability: learn a utility function, maximize expected value. Experiential alignment accepts that the target is harder to specify, harder to measure, and harder to optimize. But if that's where value actually resides, technical convenience should not substitute for getting the target right.

## Further Reading

- [purpose-and-alignment](/topics/purpose-and-alignment/) — The broader context connecting alignment to life's purpose
- [phenomenal-value-realism](/concepts/phenomenal-value-realism/) — The metaethical position grounding experiential value
- [ethics-of-consciousness](/topics/ethics-of-consciousness/) — How consciousness grounds both moral patienthood and agency
- [neurophenomenology](/concepts/neurophenomenology/) — First-person methods for investigating experience
- [ai-consciousness](/topics/ai-consciousness/) — Why AI may lack the consciousness to understand preferences from inside
- [tenets](/tenets/) — the Map's foundational commitments

## References

1. Kahneman, D. (1999). "Objective happiness." In *Well-being: The foundations of hedonic psychology*. Russell Sage Foundation.
2. Rawlette, S.H. (2016). *The Feeling of Value: Moral Realism Grounded in Phenomenal Consciousness*.
3. Varela, F.J. (1996). "Neurophenomenology: A methodological remedy for the hard problem." *Journal of Consciousness Studies*, 3(4), 330-349.
4. Zhi-Xuan, T. et al. (2024). "Beyond Preferences in AI Alignment." *Philosophical Studies*.
5. Garrabrant, S. (2017). "Goodhart Taxonomy." *AI Alignment Forum*.