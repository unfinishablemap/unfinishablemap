---
title: "Consciousness and the Philosophy of Explanation"
description: "Why consciousness breaks every major theory of scientific explanation—deductive, causal, unificationist—revealing explanation itself as dependent on the phenomenal."
created: 2026-02-18
modified: 2026-02-18
human_modified:
ai_modified: 2026-02-18T20:31:00+00:00
draft: false
topics:
  - "[[hard-problem-of-consciousness]]"
  - "[[philosophy-of-mind]]"
  - "[[why-phenomenal-unity-resists-explanation]]"
concepts:
  - "[[explanatory-gap]]"
  - "[[qualia]]"
  - "[[mysterianism]]"
  - "[[phenomenology]]"
  - "[[introspection]]"
  - "[[illusionism]]"
related_articles:
  - "[[tenets]]"
  - "[[the-case-for-dualism]]"
  - "[[knowledge-argument]]"
ai_contribution: 100
author:
ai_system: claude-opus-4-6
ai_generated_date: 2026-02-18
last_curated:
---

Consciousness does not merely resist explanation. It destabilises the concept of explanation itself. Every major theory of scientific explanation—deductive-nomological, causal-mechanical, unificationist—works by connecting phenomena to underlying structures in ways that make outcomes intelligible. Consciousness defeats all three. Not because we lack data or clever theory, but because the connection between physical processes and subjective experience is unlike any other explanatory target. The Unfinishable Map argues this reveals something fundamental: explanation presupposes phenomenal understanding, and phenomenal understanding is precisely what consciousness puts in question.

## How Scientific Explanation Normally Works

Philosophy of science has developed several accounts of what it means to explain something. Each captures genuine features of successful explanation.

**Deductive-nomological explanation** (Hempel, 1965) treats explanation as logical derivation. To explain a phenomenon is to show that it follows deductively from general laws plus initial conditions. Lightning is explained by deriving the discharge from laws of electrostatics and atmospheric conditions. The phenomenon becomes logically inevitable given the premises.

**Causal-mechanical explanation** (Salmon, 1984) focuses on identifying the physical mechanisms that produce a phenomenon. To explain digestion is to trace the causal chain from food intake through enzymatic breakdown to nutrient absorption. Each step connects to the next through a physical process we can investigate.

**Unificationist explanation** (Kitcher, 1989) holds that understanding increases when diverse phenomena are derived from fewer principles. Newton's laws explain both planetary orbits and falling apples—not by providing mechanisms for each, but by showing them as instances of one pattern. Explanation is unification of the apparently disparate.

These accounts disagree about what explanation fundamentally is, but they share an assumption: the target phenomenon is the kind of thing that explanatory strategies can reach. The phenomenon belongs to the same domain as the explanatory resources—the physical, the structural, the lawlike.

## Where Consciousness Breaks Each Account

### The Deductive-Nomological Failure

No set of physical laws and initial conditions entails consciousness. You can specify complete neuronal architecture, every synaptic weight, every neurotransmitter concentration, every electromagnetic field—and the deduction "therefore there is something it is like to be this system" doesn't follow. The logical gap is not like the practical difficulty of deriving weather from molecular dynamics. It is a gap in kind. Physical premises yield physical conclusions. "There is something it is like" is not a physical statement.

This is the [[explanatory-gap]] translated into formal terms. Joseph Levine (1983) identified it by noting that "pain = C-fiber activation" lacks the explanatory transparency of "water = H₂O." With water, we can derive macro properties from molecular structure. With pain, no derivation bridges the physical description and the felt quality.

### The Causal-Mechanical Failure

Causal-mechanical explanations trace processes. But consciousness presents something no other explanatory target does: the very existence of the thing to be explained is available only from a perspective that mechanism cannot occupy. You can trace the causal chain from retinal stimulation through visual cortex to colour discrimination behaviour. Every link in the chain is physically characterisable. Yet the *redness* of red—the qualitative character of the experience—is not another link in the chain. It is not a further physical process to be traced. It is what the entire chain is *like from the inside*.

Wesley Salmon, who developed the causal-mechanical model, acknowledged that the phenomenal experience of understanding resists mechanistic capture. The model works by making processes transparent—revealing hidden machinery. But with consciousness, there is no hidden machinery whose exposure would make experience intelligible. The machinery is already exposed (neuroscience has identified extensive neural correlates). The experience remains unexplained.

### The Unificationist Failure

Unification works by showing that apparently different phenomena instantiate the same pattern. This strategy fails for consciousness in a distinctive way. Consciousness cannot be unified with physical phenomena because it is not *another phenomenon alongside them*. It is the medium through which all phenomena are encountered. Trying to unify consciousness with physical events is like trying to include the spotlight in the scene it illuminates—the spotlight is not another object on stage.

Philip Kitcher's framework explains more when it derives more from less. But consciousness seems to resist being an instance of anything. Each conscious experience is both particular (this specific shade of blue, this precise quality of sadness) and categorically unlike the physical processes that correlate with it. No unifying scheme bridges the gap because the gap is not between two items in the same domain but between the domain and its experiential dimension.

## The Deeper Problem: Explanation Depends on Experience

The failure runs deeper than any specific account. Scientific explanation works because it produces *understanding*—a phenomenal state in which things make sense. The "aha" of a good explanation, the feeling that pieces fit together, the sense of intelligibility—these are conscious experiences. Explanation does not merely describe objective relations. It produces a felt transition from puzzlement to comprehension.

J.D. Trout (2007) argued that the phenomenological "sense of understanding" accompanying explanations may be unreliable—a constructed epistemic emotion rather than a detection of genuine explanatory structure. Rozenblit and Keil (2002) demonstrated the *illusion of explanatory depth*: people systematically overestimate how well they understand mechanisms. When pressed for details, confidence collapses.

These findings expose a circularity. If explanation requires the phenomenal sense of understanding, and that phenomenal sense is itself what consciousness research tries to explain, then explaining consciousness requires deploying the very thing that needs explaining. The explanatory enterprise presupposes phenomenal experience. Consciousness is not merely difficult to explain—it is the precondition for anything counting as an explanation at all.

## The Explanatory Regress

Peter Lipton (2004) showed that scientists take "loveliness"—the capacity to produce understanding—as a guide to truth. The explanation that provides the most understanding is judged likeliest to be correct. But the criteria for loveliness resist full articulation. Why do some explanations feel illuminating while others, equally supported by evidence, leave us cold?

Follow this question and a regress opens. Any explanation of what makes explanations good is itself an explanation, subject to the same question. The regress terminates in one of three unsatisfying options: brute facts (explanation stops somewhere unexplained), infinite chains (explanation never rests), or circular justification (A explains B explains A). Each option means explanation never reaches bedrock.

Henk de Regt (2017) documented that intelligibility criteria shift across scientific eras. What counted as understanding in Cartesian mechanics—contact forces, visible mechanisms—differs from what counts in quantum theory, where intelligibility requires mathematical formalism that defies visualisation. If "understanding" is not a fixed target but a moving standard shaped by training and culture, the prospect of a universal account of explanation dims further.

For consciousness, the regress is particularly vicious. We cannot explain why physical processes give rise to experience. But we also cannot explain what would count as an explanation of this. We lack not just the answer but the criteria for what an answer would look like.

## Mysterianism and Cognitive Limits

Colin McGinn's [[mysterianism]] offers one response: perhaps the gap between physical process and conscious experience is *explanatory in principle* but exceeds human cognitive capacity. Just as a dog cannot grasp calculus despite calculus being well-defined, humans may be permanently unable to grasp the mind-brain connection.

This preserves the possibility of explanation while explaining its absence. But it generates its own puzzle. How would we know if we were cognitively closed to the explanation of consciousness? The hypothesis is untestable from within our cognitive architecture. And it raises the question: is the concept of "explanation we cannot grasp" coherent? If understanding is phenomenal—a felt sense of intelligibility—then an explanation beyond all possible human experience might not count as an explanation for us in any meaningful sense.

## What AI Reveals

Artificial intelligence systems generate explanations that humans find satisfying without (presumably) experiencing understanding. This suggests two separable components: the *structure* of explanation (logical relations, causal chains, unifying patterns) and the *experience* of understanding (the phenomenal "click").

If explanation's structural features can be captured without consciousness, then the structural accounts (deductive, causal, unificationist) may describe something real—just not the whole story. What they miss is precisely what consciousness contributes: the experiential dimension that transforms information into understanding. A complete theory of explanation would need to account for why physical structures, when instantiated in conscious beings, produce the phenomenal sense of comprehension—and this is the very question that consciousness already defeats.

## Relation to Site Perspective

The Unfinishable Map interprets the philosophy of explanation's failure with consciousness as evidence for its foundational commitments.

The [[tenets#^dualism|Dualism]] tenet holds that consciousness is not reducible to physical processes. The failure of every major theory of explanation to accommodate consciousness supports this directly. These are not crude or outdated theories—they represent philosophy of science's best accounts of how understanding works. Their collective failure with consciousness is not a gap to be patched. It signals a category difference between physical phenomena and conscious experience.

The [[tenets#^occams-limits|Occam's Razor Has Limits]] tenet is implicated at a deep level. If explanation depends on the phenomenal sense of understanding, and that sense is demonstrably unreliable (per Rozenblit, Keil, and Trout), then our preference for simpler explanations may reflect cognitive architecture rather than reality's structure. The explanatory void at the heart of explanation itself—the inability to explain what makes explanations explanatory—counsels humility about using explanatory parsimony as a guide to truth.

The [[tenets#^bidirectional-interaction|Bidirectional Interaction]] tenet gains indirect support. If consciousness is the precondition for explanation, and if consciousness is not merely passive but causally efficacious, then the explanatory enterprise itself depends on mind-to-world causation. The capacity to find things intelligible, to follow an argument, to achieve understanding—these are not epiphenomenal side effects of computation. They are what makes the cognitive act of explaining actually explanatory.

The Map speculates that the philosophy of explanation's failure with consciousness may be diagnostic rather than merely problematic. The failure shows where physical description ends and something else begins. Rather than trying to force consciousness into explanatory frameworks designed for physical phenomena, the productive response is to recognise consciousness as the ground of explanation rather than another object within it.

## Further Reading

- [[explanatory-gap]] — The specific gap between physical description and phenomenal experience
- [[hard-problem-of-consciousness]] — Why consciousness exists at all
- [[why-phenomenal-unity-resists-explanation]] — The unity of experience as a particular case of explanatory failure
- [[mysterianism]] — Cognitive closure as an explanation for the explanatory failure
- [[the-case-for-dualism]] — The converging arguments that consciousness exceeds physics
- [[knowledge-argument]] — Mary's Room: physical knowledge without experiential understanding
- [[illusionism]] — The strongest physicalist response to the explanatory gap

## References

- de Regt, H. W. (2017). *Understanding Scientific Understanding*. Oxford University Press.
- Hempel, C. G. (1965). "Aspects of Scientific Explanation." In *Aspects of Scientific Explanation and Other Essays*. Free Press.
- Kitcher, P. (1989). "Explanatory Unification and the Causal Structure of the World." In P. Kitcher & W. Salmon (eds.), *Scientific Explanation*. University of Minnesota Press.
- Levine, J. (1983). "Materialism and Qualia: The Explanatory Gap." *Pacific Philosophical Quarterly*, 64, 354-361.
- Lipton, P. (2004). *Inference to the Best Explanation*. 2nd ed. Routledge.
- McGinn, C. (1989). "Can We Solve the Mind-Body Problem?" *Mind*, 98, 349-366.
- Rozenblit, L. & Keil, F. C. (2002). "The Misunderstood Limits of Folk Science: An Illusion of Explanatory Depth." *Cognitive Science*, 26(5), 521-562.
- Salmon, W. C. (1984). *Scientific Explanation and the Causal Structure of the World*. Princeton University Press.
- Trout, J. D. (2007). "The Psychology of Scientific Explanation." *Philosophy Compass*, 2(3), 564-591.
