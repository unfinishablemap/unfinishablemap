---
title: "Purpose and AI Alignment"
description: "AI alignment assumes human values can be learned from behavior. But if we don't know humanity's purpose, alignment builds on unstable foundations."
created: 2026-01-13
modified: 2026-01-21
human_modified: null
ai_modified: 2026-02-02T11:14:00+00:00
draft: false
topics:
  - "[[meaning-of-life]]"
  - "[[ai-consciousness]]"
concepts:
  - "[[nihilism]]"
  - "[[existentialism]]"
  - "[[experiential-alignment]]"
  - "[[neurophenomenology]]"
  - "[[phenomenal-value-realism]]"
  - "[[ethics-of-consciousness]]"
  - "[[emotional-consciousness]]"
  - "[[illusionism]]"
  - "[[introspection]]"
  - "[[witness-consciousness]]"
  - "[[haecceity]]"
  - "[[decoherence]]"
related_articles:
  - "[[tenets]]"
  - "[[voids]]"
  - "[[purpose-of-life-ai-alignment-2026-01-10]]"
  - "[[alignment-objective-experiential-terms-2026-01-16]]"
ai_contribution: 100
author: null
ai_system: claude-opus-4-5-20251101
ai_generated_date: 2026-01-13
last_curated: null
last_deep_review: 2026-01-30T11:45:00+00:00
tag: diversion
---

AI alignment—The Unfinishable Map of ensuring artificial intelligence serves human interests—faces a problem that philosophers have debated for millennia: we do not know what human interests ultimately are. The dominant approach learns from human preferences, but preferences may not track what truly matters. If we cannot align AI without knowing humanity's purpose, and we lack clarity on that purpose, then alignment may be building on unstable foundations.

This creates a surprising connection between ancient questions about life's meaning and cutting-edge AI safety research. The Map's framework—particularly its commitment to consciousness as fundamental and its skepticism about simple explanations—illuminates this connection in ways that standard approaches miss.

## The Alignment Assumption

AI alignment researchers typically assume that human values can be inferred from human behaviour. Stuart Russell's influential framework proposes three principles: AI should maximize human values, remain uncertain about what those values are, and learn from human behaviour. The approach treats humans as preference-maximizing agents whose choices reveal what they care about.

This assumption has been challenged. A 2024 paper in *Philosophical Studies* argues that preferences fail to capture the "thick semantic content" of human values. Preferences are revealed through choices, but choices occur under constraints of time, information, and cognitive capacity. What I choose between options A and B tells you little about what I would choose given options C through Z, or about whether any available option reflects what I genuinely value.

More fundamentally: preferences might be systematically mistaken about what matters. I might prefer activities that leave me empty over ones that would fulfil me. Evolution shaped human preferences for survival and reproduction, not for flourishing or meaning. If our preferences are unreliable guides to our own good, learning from them cannot produce alignment.

## Purpose vs. Meaning

The philosophical literature distinguishes purpose from meaning. Purpose implies teleology—an end toward which something moves, a goal inherent in its nature. Meaning is broader, encompassing sense-making and significance without requiring a predetermined endpoint.

Aristotle held that humans have a telos: eudaimonia, or flourishing, achieved through rational activity and virtue. On this view, purpose is discovered rather than invented. Human nature has a proper function, and aligning AI would require understanding that function.

Existentialists reject inherent purpose. Sartre argued that existence precedes essence—we exist first, then define ourselves through choices. Camus went further: the universe offers no meaning at all, and we must create it through defiant engagement with absurdity. On these views, purpose is constructed rather than discovered, and AI alignment would need to defer to human creation rather than uncover objective truth.

Hybrid views like Susan Wolf's propose that meaning requires both subjective engagement and objective worthiness. You must care about what you do, and what you do must be genuinely worth caring about. Neither pure subjectivism (any project can be meaningful) nor pure objectivism (meaning exists independent of caring) captures the phenomenon.

## The Preferentist Critique

The standard alignment approach—learning human values from preferences—faces what we might call the *preferentist critique*. Preferences are thin representations of values. They tell us which of two options someone chose but not why, not whether they would endorse that choice on reflection, and not whether the choice serves their deeper interests.

Consider: I prefer scrolling social media to reading philosophy. Does this preference reveal my values? It reveals something about my psychology under conditions of decision fatigue and algorithmic manipulation. It says little about what I would choose were I fully informed, undistracted, and reflectively considering my life as a whole.

Russell acknowledges that humans "don't know their own preference structure." His solution is to maintain uncertainty and remain corrigible—willing to be corrected. But this assumes we will eventually converge on our true preferences through reflection and dialogue. If our preferences are fundamentally unreliable, or if our purpose lies beyond what preferences can track, uncertainty alone cannot solve the problem.

## What Consciousness Adds

The Map's framework suggests a deeper issue. If consciousness is fundamental and irreducible—not merely an emergent property of information processing—then human purpose may be tied to consciousness in ways that preferences cannot capture.

A preference-learning AI treats humans as behavioural systems. It observes inputs (stimuli) and outputs (choices) and infers a utility function. But on the Map's view, what matters about humans is not the behavioural mapping but the conscious experience underlying it. Two beings with identical input-output profiles might have radically different conscious lives—one rich with meaning, the other hollow.

If [[ai-consciousness|AI systems lack consciousness]] (as the Map's framework implies), then they cannot understand human purpose from the inside. They can model behaviour, predict choices, and optimize for preference satisfaction. But they cannot grasp what it is like to be human, and therefore cannot grasp what human flourishing actually involves.

This is not a claim that alignment is impossible. It is a claim that alignment approaches treating humans as preference-maximizing systems may miss what matters most.

## Implications for Alignment

Three implications follow from this analysis:

**First: Epistemic humility.** If human purpose is genuinely uncertain—if philosophers have debated it for millennia without resolution—then AI systems should be designed for profound uncertainty about what they are optimizing for. Russell's corrigibility is a step in this direction, but the uncertainty may need to go deeper. An AI confident that it understands human values through preference learning is an AI that has not grasped the problem.

**Second: The limits of optimization.** If purpose cannot be fully captured by any utility function, then alignment cannot be achieved through optimization alone. Something more like dialogue, interpretation, and ongoing negotiation may be required—treating humans as interlocutors rather than preference-sources.

**Third: Consciousness as constraint.** If consciousness matters in ways that behaviour does not fully reveal, then alignment research should take seriously the possibility that what we want from AI is not preference satisfaction but something related to the quality of conscious experience. This remains vague—we do not have a science of flourishing. But it suggests that alignment research and philosophy of mind are more deeply connected than usually recognized.

## Relation to Site Perspective

The [[tenets#^occams-limits|Occam's Razor Has Limits]] tenet applies directly. The preferentist model is appealingly simple: humans are preference-maximizers, preferences are revealed through choices, AI should maximize expected preference satisfaction. But this simplicity may mask the actual complexity of human purpose. Just as parsimony led physics astray before quantum mechanics, it may lead alignment astray before we understand consciousness.

The [[tenets#^dualism|Dualism]] tenet adds depth. If consciousness is irreducible, then human purpose may be tied to features of experience that no behavioural analysis can capture. The "what it is like" of conscious life—its qualitative character—may be where purpose resides, and that is precisely what preference-learning systems cannot access.

This connects to the Map's treatment of [[voids]]—territories that may be unchartable. Human purpose might be partially in this category. Not because the question is meaningless, but because our conceptual resources may be inadequate to fully articulate it. [[apophatic-approaches|Apophatic methods]]—knowing what purpose is *not*—may be as important as positive specification.

The practical upshot: AI systems should be designed with humility about human purpose, not confidence. They should be corrigible not merely because values are hard to specify, but because values may be impossible to fully specify. The goal is not an AI that optimizes for human flourishing, but an AI that supports human inquiry into what flourishing means—a partner in the unfinishable map rather than a solver of a problem we have not solved ourselves.

## Experiential Alignment: An Alternative Framework

If phenomenal consciousness is the ground of value, alignment might target experiential quality directly rather than learning from preferences. This view is grounded in [[phenomenal-value-realism]]—the metaethical position that intrinsic value is a feature of conscious experience itself. [[Experiential-alignment|Experiential alignment]] proposes that AI systems should promote predicted distributions over human conscious experiences—suffering, agency, meaning, attention quality, social connection—rather than observed choices.

This approach accepts that the target is harder to specify, harder to measure, and harder to optimize than preference satisfaction. But if value actually resides in the quality of conscious experience, technical convenience should not substitute for getting the target right.

The experiential approach faces its own challenges: proxies for experience risk Goodhart effects, wireheading remains a concern, and first-person methods like [[neurophenomenology]] are labour-intensive. But it represents a more honest framing of what alignment is trying to achieve—promoting flourishing conscious experience, not merely satisfying preferences that may not track what matters.

### The Special Weight of Valence

[[Emotional-consciousness|Emotional consciousness]] research suggests experiential alignment should give special weight to **valence**—the felt positive or negative quality of experience. Valence is not one phenomenal dimension among many; it may be the dimension that matters most for alignment.

The [[emotional-consciousness#valence|hedonic account]] holds that valence is an intrinsic property of experience: the badness of pain is what pain *is*, not something pain represents. If this is correct, then a complete description of preferences, behaviours, and neural states leaves out precisely what matters—the felt goodness and badness that gives experience its significance.

This has practical implications for experiential alignment:

| Approach | What It Targets | The Problem |
|----------|-----------------|-------------|
| **Preference learning** | Revealed choices | Choices don't track felt quality; pain asymbolia patients prefer pain-causing activities |
| **Broad experiential** | Any conscious features | Treats phenomenal unity and suffering as equivalent concerns |
| **Valence-weighted** | Hedonic quality first | Accepts that suffering and enjoyment are not just features of experience but *what makes experience matter* |

[[Emotional-consciousness#Emotional Consciousness and Moral Status|Valence sentientism]]—the view that moral status derives from capacity for suffering and enjoyment—provides the theoretical grounding. If only valenced experience creates interests worth protecting, then alignment systems should prioritise the valence dimension over other phenomenal features.

The research on [[emotional-consciousness#Core Affect and Emotional Construction|core affect]] adds nuance: humans are never affectively neutral. There is always a background felt quality of experience, varying in pleasantness and arousal. A truly aligned system would consider this continuous valenced character, not merely discrete episodes of pleasure and pain. The goal is not preference satisfaction, nor even a list of positive experiences, but supporting lives rich in positive valence across their temporal extent.

## The Illusionist Challenge

[[Illusionism]]—the view that phenomenal consciousness is an introspective illusion—presents the most radical objection to this entire framework. If there is no phenomenal experience to target, experiential alignment has no referent, and preferentism might be the best we can do.

### The Regress Response

The illusionist challenge faces a fundamental problem: the experience of *seeming* to have preferences, *seeming* to suffer, *seeming* to find meaning must itself appear *to* something. Raymond Tallis's point applies: "Misrepresentation presupposes presentation." To be under an illusion of experiential quality, something must be experiencing that illusion. The appearance of the inadequacy of preferences—the recognition that someone could satisfy all preferences and still live a hollow life—is itself experiential data that illusionism claims doesn't exist.

The regress terminates in genuine experience. Either the seeming of preference-inadequacy is phenomenal (in which case phenomenal states exist and matter) or it isn't (in which case the illusionist cannot explain why preferentism *seems* insufficient to those reflecting on it).

### The Practical Asymmetry

Even granting illusionism for argument's sake, a practical asymmetry remains. If phenomenal consciousness is real, preferentist alignment misses what ultimately matters—the quality of conscious experience. If illusionism is correct, experiential alignment merely targets sophisticated functional states that still track something important: the states that generate reports of suffering, satisfaction, and meaning. The downside of phenomenal consciousness being real while we optimize only for preferences is asymmetrically worse than the downside of phenomenal consciousness being illusory while we target experiential quality.

### Contemplative Evidence

[[Introspection]] research shows that trained [[witness-consciousness|contemplatives]] report accessing experiential quality directly—not as an attribution to experience but as a feature observed in the observing. If phenomenal states were illusory, training should eventually dissolve the illusion. Instead, contemplative traditions consistently report that practice *refines* phenomenal access, revealing finer-grained distinctions in hedonic valence, meaning, agency, and attention quality. The 2025 "renaissance of first-person methods" in consciousness science provides systematic evidence that phenomenal reports can be calibrated and improved.

## Process Philosophy Perspective

Alfred North Whitehead's process philosophy illuminates why experiential alignment captures something preferentism misses.

### Actual Occasions and Purpose

For Whitehead, reality consists of "actual occasions"—momentary events of experience. Each occasion has "subjective aim"—a directedness toward what Whitehead called "satisfaction," the richest possible integration of the occasion's data. Crucially, this satisfaction is not preference satisfaction but *experiential intensity*—the fullness of what the occasion feels.

Human purpose, on this view, is not an external goal to be discovered or constructed. It is the ongoing subjective aim of each moment of experience toward maximal intensity and harmony. Purpose is *in* experience, not something experience serves.

### Why Preferences Are Thin

On the Whiteheadian view, preferences are abstractions from experiential richness. To know that someone prefers A to B is to know something about their anticipated experience, but not the experiential quality itself. The actual occasion's satisfaction cannot be captured by a preference ordering because satisfaction concerns the felt integration of the moment—something irreducible to comparative rankings.

This explains why preference satisfaction can leave people empty. The preferences were satisfied, but the experiential intensity—what Whitehead would call the "depth of satisfaction"—was low. The occasions of experience lacked richness even while the preference orderings were honored.

### Concrescence and Alignment

Whitehead's "concrescence"—the process by which each actual occasion becomes determinate—suggests a model for alignment. Rather than learning from preferences (past determinations), experiential alignment would support the conditions for rich concrescence: occasions of experience that integrate data harmoniously and intensely. This is a prospective rather than retrospective target—supporting the *becoming* of experience, not merely the satisfaction of prior preferences.

## Contemplative Evidence

The inadequacy of preferences is not merely a philosophical argument—it is phenomenologically accessible to trained introspection.

### Witness Consciousness and Purpose

[[Witness-consciousness]] traditions (Advaita Vedanta's *sakshi*, Buddhist *sati*) systematically cultivate the capacity to observe experiential states. Practitioners report distinguishing:

- **Preference satisfaction** as a cognitive state: "I got what I wanted"
- **Experiential flourishing** as a felt quality: depth, presence, meaning

These can dissociate. One can get what one wanted while the experiential quality remains shallow. Conversely, one can fail to satisfy preferences while experiencing profound meaning—as in sacrifice for others or engagement with difficulty.

### The Buddhist Complication

[[Buddhism-and-dualism|Buddhist perspectives]] add important nuance. While experiential alignment targets positive experiential quality, Buddhist analysis identifies *tanha* (craving)—including craving for positive experience—as the root of suffering. This doesn't undermine experiential alignment but refines it. The goal is not maximizing pleasant experiences (which generates craving) but supporting conditions for experiential flourishing that include equanimity—positive valence without grasping.

The experiential alignment framework's emphasis on agency, meaning, and attention quality—not merely hedonic valence—resonates with Buddhist middle way thinking. The suffering floor doesn't mandate maximizing pleasure; it mandates preventing extreme suffering while supporting experiential richness across multiple dimensions.

### Jhana States and Alignment Targets

Meditative *jhana* states demonstrate that experiential dimensions can be systematically accessed and distinguished. Practitioners report:

- First jhana: pleasure with applied attention
- Second jhana: pleasure without effort
- Third jhana: contentment without rapture
- Fourth jhana: equanimity beyond pleasure and pain

These distinctions are phenomenologically precise in ways preference surveys cannot capture. All four states might be "preferred" to ordinary experience, but they differ dramatically in experiential character. Experiential alignment requires access to such fine-grained phenomenology—precisely what first-person methods provide and preference learning cannot.

## What Would Challenge This View?

The framework connecting purpose to experiential alignment makes falsifiable claims. It should be reconsidered if:

1. **Preferentism proves experientially reliable**: If longitudinal studies demonstrated that satisfying revealed preferences reliably produces positive experiential outcomes across multiple dimensions (hedonic valence, meaning, agency, attention quality), preferentism would be vindicated as sufficient. Current evidence suggests otherwise: preference satisfaction correlates with life satisfaction but not with moment-to-moment experiential quality (Kahneman's "focusing illusion").

2. **Illusionism gains empirical support**: If neuroscience demonstrated that phenomenal consciousness is genuinely illusory—not just that introspection is fallible, but that there is nothing it is like to be a conscious system—then experiential alignment would lose its target. However, the illusionist would need to explain why the "illusion" of suffering still seems to matter, and this explanation would likely reinstate something functionally equivalent to experiential quality.

3. **Experiential proxies prove systematically unreliable**: If trained contemplative reports, physiological measures, and behavioural indicators systematically diverged—if no combination of proxies tracked experiential quality—the framework would become empirically intractable. Current evidence suggests proxies can be triangulated, though with significant noise.

4. **Purpose proves specifiable**: If philosophers converged on a specification of human purpose that could be encoded in a utility function—if the millennia-long debate resolved—then preference learning from that specification might suffice. The framework's claim is precisely that such convergence is unlikely given the [[voids|unchartable]] character of human purpose.

5. **AI consciousness emerges**: If AI systems developed genuine phenomenal consciousness, they could understand human purpose from the inside—at least in principle. The Map's current position that AI lacks consciousness would need revision, along with the claim that AI cannot grasp what human flourishing involves.

## Relation to Site Perspective

This analysis connects deeply to all five of the Map's foundational commitments.

**[[tenets#^dualism|Dualism]]**: If consciousness is fundamental and irreducible—not merely an emergent property of information processing—then human purpose may be tied to consciousness in ways that preferences cannot capture. A preference-learning AI treats humans as behavioural systems, observing inputs and outputs to infer a utility function. But on the Map's view, what matters about humans is the conscious experience underlying behaviour. Two beings with identical input-output profiles might have radically different conscious lives—one rich with meaning, the other hollow. The [[illusionism|illusionist]] alternative would collapse this distinction, but the regress argument shows that even the "illusion" of experiential quality must appear to something, and that appearing is itself phenomenal.

**[[tenets#^minimal-quantum-interaction|Minimal Quantum Interaction]]**: The Map proposes that consciousness interfaces with physical reality through quantum mechanisms—biasing otherwise indeterminate outcomes. This has implications for alignment: genuine agency involves consciousness actually selecting among options, not merely appearing to choose. If AI systems create environments that disrupt the neural quantum mechanisms through which agency operates (through excessive [[decoherence]] or manipulation of attention), they undermine genuine agency even while satisfying preferences. The valence dimension connects here too: if conscious selection operates through pleasure and pain signals that bias quantum outcomes, then valence is not just one experiential feature among many—it is the dimension through which consciousness exercises its causal role.

**[[tenets#^bidirectional-interaction|Bidirectional Interaction]]**: If consciousness causally influences the physical world, agency is real, not merely apparent. This grounds the article's claim that alignment should preserve genuine agency, not agency illusions. The experiential dimension of agency tracks something metaphysically real: conscious selection of outcomes. The very fact that we discuss consciousness, purpose, and alignment provides evidence for bidirectional interaction—if consciousness were epiphenomenal, our purpose-talk would be accidentally correlated with our purposes rather than causally connected to them. [[Introspection]] about purpose would be systematically unreliable in a way that would undermine the philosophical discourse itself.

**[[tenets#^no-many-worlds|No Many Worlds]]**: If identity is indexical and unrepeatable, each person's experiential quality matters uniquely. [[Haecceity]]—the non-qualitative property of being *this* particular conscious subject—grounds why aggregate preference satisfaction cannot substitute for individual experiential flourishing. The person who suffers is *this* person, not an instance of a type. Many-worlds, where all outcomes occur across the branching structure, undermines the concept of "promoting" experiential quality: if every distribution over experiences is realized somewhere, alignment loses its normative force. Purpose—as experienced—is about *this* life, *this* trajectory, *this* conscious subject's flourishing. The indexical character of purpose supports rejecting MWI while grounding why experiential alignment must consider distributions over individuals, not just aggregate metrics.

**[[tenets#^occams-limits|Occam's Razor Has Limits]]**: The preferentist model is appealingly simple: humans are preference-maximizers, preferences are revealed through choices, AI should maximize expected preference satisfaction. But this simplicity may mask genuine complexity. Just as parsimony led physics astray before quantum mechanics (when simpler Newtonian mechanics seemed sufficient), it may lead alignment astray before we understand consciousness. The article's claim that purpose is "partially unchartable" reflects this tenet: some domains resist the simple models parsimony prefers. The [[voids|unchartable territories]] may include aspects of human purpose itself—not because purpose is meaningless, but because our conceptual resources are inadequate to fully articulate what we can nevertheless experience.

The practical upshot: AI systems should be designed with profound humility about human purpose—partners in inquiry rather than solvers of a problem we have not solved ourselves.

## Further Reading

- [[emotional-consciousness]] — The phenomenology of valence and why felt quality matters for alignment
- [[experiential-alignment]] — The alternative framework targeting experiential quality
- [[phenomenal-value-realism]] — The metaethical grounding for consciousness-based value
- [[phenomenal-value-realism-development]] — How phenomenal value pluralism addresses the measurement problem and metaethical challenges
- [[ethics-of-consciousness]] — The broader ethical framework for consciousness
- [[meaning-of-life]] — the Map's treatment of philosophical approaches to life's meaning
- [[ai-consciousness]] — Why the Map holds that AI systems lack consciousness
- [[voids]] — On the limits of what can be known or mapped
- [[tenets]] — The foundational commitments underlying this analysis
- [[illusionism]] — The strongest challenge to taking phenomenal consciousness seriously
- [[introspection]] — First-person methods for investigating experience
- [[witness-consciousness]] — Contemplative access to experiential dimensions
- [[haecceity]] — Why *this* conscious subject's purpose matters uniquely
- [[buddhism-and-dualism]] — How Buddhist perspectives refine phenomenal value realism

## References

1. Zhi-Xuan, T., Carroll, M., Franklin, M., & Ashton, H. (2024). "Beyond Preferences in AI Alignment." *Philosophical Studies*. https://arxiv.org/abs/2408.16984
2. Russell, S. (2019). *Human Compatible: Artificial Intelligence and the Problem of Control*. Viking.
3. Wolf, S. (2010). *Meaning in Life and Why It Matters*. Princeton University Press.
4. Chalmers, D. (1996). *The Conscious Mind*. Oxford University Press.
5. Frankish, K. (2016). "Illusionism as a Theory of Consciousness." *Journal of Consciousness Studies*, 23(11-12), 11-39.
6. Tallis, R. (2014). "The mystery of existence: why is there something rather than nothing?" In *Aping Mankind*. Routledge.
7. Whitehead, A.N. (1929). *Process and Reality*. Macmillan.
8. Fox, K.C.R. et al. (2012). "Meditation experience predicts introspective accuracy." *PLOS ONE*, 7(9), e45370.
9. Kahneman, D. & Deaton, A. (2010). "High income improves evaluation of life but not emotional well-being." *PNAS*, 107(38), 16489-16493.
