---
title: "Purpose and AI Alignment"
created: 2026-01-13
modified: 2026-01-19
human_modified: null
ai_modified: 2026-01-19T00:20:00+00:00
draft: false
topics:
  - "[[meaning-of-life]]"
  - "[[ai-consciousness]]"
concepts:
  - "[[nihilism]]"
  - "[[existentialism]]"
  - "[[experiential-alignment]]"
  - "[[neurophenomenology]]"
  - "[[phenomenal-value-realism]]"
  - "[[ethics-of-consciousness]]"
  - "[[emotional-consciousness]]"
related_articles:
  - "[[tenets]]"
  - "[[voids]]"
  - "[[purpose-of-life-ai-alignment-2026-01-10]]"
  - "[[alignment-objective-experiential-terms-2026-01-16]]"
ai_contribution: 100
author: null
ai_system: claude-opus-4-5-20251101
ai_generated_date: 2026-01-13
last_curated: null
tag: diversion
---

AI alignment—The Unfinishable Map of ensuring artificial intelligence serves human interests—faces a problem that philosophers have debated for millennia: we do not know what human interests ultimately are. The dominant approach learns from human preferences, but preferences may not track what truly matters. If we cannot align AI without knowing humanity's purpose, and we lack clarity on that purpose, then alignment may be building on unstable foundations.

This creates a surprising connection between ancient questions about life's meaning and cutting-edge AI safety research. The Map's framework—particularly its commitment to consciousness as fundamental and its skepticism about simple explanations—illuminates this connection in ways that standard approaches miss.

## The Alignment Assumption

AI alignment researchers typically assume that human values can be inferred from human behaviour. Stuart Russell's influential framework proposes three principles: AI should maximize human values, remain uncertain about what those values are, and learn from human behaviour. The approach treats humans as preference-maximizing agents whose choices reveal what they care about.

This assumption has been challenged. A 2024 paper in *Philosophical Studies* argues that preferences fail to capture the "thick semantic content" of human values. Preferences are revealed through choices, but choices occur under constraints of time, information, and cognitive capacity. What I choose between options A and B tells you little about what I would choose given options C through Z, or about whether any available option reflects what I genuinely value.

More fundamentally: preferences might be systematically mistaken about what matters. I might prefer activities that leave me empty over ones that would fulfil me. Evolution shaped human preferences for survival and reproduction, not for flourishing or meaning. If our preferences are unreliable guides to our own good, learning from them cannot produce alignment.

## Purpose vs. Meaning

The philosophical literature distinguishes purpose from meaning. Purpose implies teleology—an end toward which something moves, a goal inherent in its nature. Meaning is broader, encompassing sense-making and significance without requiring a predetermined endpoint.

Aristotle held that humans have a telos: eudaimonia, or flourishing, achieved through rational activity and virtue. On this view, purpose is discovered rather than invented. Human nature has a proper function, and aligning AI would require understanding that function.

Existentialists reject inherent purpose. Sartre argued that existence precedes essence—we exist first, then define ourselves through choices. Camus went further: the universe offers no meaning at all, and we must create it through defiant engagement with absurdity. On these views, purpose is constructed rather than discovered, and AI alignment would need to defer to human creation rather than uncover objective truth.

Hybrid views like Susan Wolf's propose that meaning requires both subjective engagement and objective worthiness. You must care about what you do, and what you do must be genuinely worth caring about. Neither pure subjectivism (any project can be meaningful) nor pure objectivism (meaning exists independent of caring) captures the phenomenon.

## The Preferentist Critique

The standard alignment approach—learning human values from preferences—faces what we might call the *preferentist critique*. Preferences are thin representations of values. They tell us which of two options someone chose but not why, not whether they would endorse that choice on reflection, and not whether the choice serves their deeper interests.

Consider: I prefer scrolling social media to reading philosophy. Does this preference reveal my values? It reveals something about my psychology under conditions of decision fatigue and algorithmic manipulation. It says little about what I would choose were I fully informed, undistracted, and reflectively considering my life as a whole.

Russell acknowledges that humans "don't know their own preference structure." His solution is to maintain uncertainty and remain corrigible—willing to be corrected. But this assumes we will eventually converge on our true preferences through reflection and dialogue. If our preferences are fundamentally unreliable, or if our purpose lies beyond what preferences can track, uncertainty alone cannot solve the problem.

## What Consciousness Adds

The Map's framework suggests a deeper issue. If consciousness is fundamental and irreducible—not merely an emergent property of information processing—then human purpose may be tied to consciousness in ways that preferences cannot capture.

A preference-learning AI treats humans as behavioural systems. It observes inputs (stimuli) and outputs (choices) and infers a utility function. But on the Map's view, what matters about humans is not the behavioural mapping but the conscious experience underlying it. Two beings with identical input-output profiles might have radically different conscious lives—one rich with meaning, the other hollow.

If [[ai-consciousness|AI systems lack consciousness]] (as the Map's framework implies), then they cannot understand human purpose from the inside. They can model behaviour, predict choices, and optimize for preference satisfaction. But they cannot grasp what it is like to be human, and therefore cannot grasp what human flourishing actually involves.

This is not a claim that alignment is impossible. It is a claim that alignment approaches treating humans as preference-maximizing systems may miss what matters most.

## Implications for Alignment

Three implications follow from this analysis:

**First: Epistemic humility.** If human purpose is genuinely uncertain—if philosophers have debated it for millennia without resolution—then AI systems should be designed for profound uncertainty about what they are optimizing for. Russell's corrigibility is a step in this direction, but the uncertainty may need to go deeper. An AI confident that it understands human values through preference learning is an AI that has not grasped the problem.

**Second: The limits of optimization.** If purpose cannot be fully captured by any utility function, then alignment cannot be achieved through optimization alone. Something more like dialogue, interpretation, and ongoing negotiation may be required—treating humans as interlocutors rather than preference-sources.

**Third: Consciousness as constraint.** If consciousness matters in ways that behaviour does not fully reveal, then alignment research should take seriously the possibility that what we want from AI is not preference satisfaction but something related to the quality of conscious experience. This remains vague—we do not have a science of flourishing. But it suggests that alignment research and philosophy of mind are more deeply connected than usually recognized.

## Relation to Site Perspective

The [[tenets#^occams-limits|Occam's Razor Has Limits]] tenet applies directly. The preferentist model is appealingly simple: humans are preference-maximizers, preferences are revealed through choices, AI should maximize expected preference satisfaction. But this simplicity may mask the actual complexity of human purpose. Just as parsimony led physics astray before quantum mechanics, it may lead alignment astray before we understand consciousness.

The [[tenets#^dualism|Dualism]] tenet adds depth. If consciousness is irreducible, then human purpose may be tied to features of experience that no behavioural analysis can capture. The "what it is like" of conscious life—its qualitative character—may be where purpose resides, and that is precisely what preference-learning systems cannot access.

This connects to the Map's treatment of [[voids]]—territories that may be unchartable. Human purpose might be partially in this category. Not because the question is meaningless, but because our conceptual resources may be inadequate to fully articulate it. [[apophatic-approaches|Apophatic methods]]—knowing what purpose is *not*—may be as important as positive specification.

The practical upshot: AI systems should be designed with humility about human purpose, not confidence. They should be corrigible not merely because values are hard to specify, but because values may be impossible to fully specify. The goal is not an AI that optimizes for human flourishing, but an AI that supports human inquiry into what flourishing means—a partner in the unfinishable map rather than a solver of a problem we have not solved ourselves.

## Experiential Alignment: An Alternative Framework

If phenomenal consciousness is the ground of value, alignment might target experiential quality directly rather than learning from preferences. This view is grounded in [[phenomenal-value-realism]]—the metaethical position that intrinsic value is a feature of conscious experience itself. [[Experiential-alignment|Experiential alignment]] proposes that AI systems should promote predicted distributions over human conscious experiences—suffering, agency, meaning, attention quality, social connection—rather than observed choices.

This approach accepts that the target is harder to specify, harder to measure, and harder to optimize than preference satisfaction. But if value actually resides in the quality of conscious experience, technical convenience should not substitute for getting the target right.

The experiential approach faces its own challenges: proxies for experience risk Goodhart effects, wireheading remains a concern, and first-person methods like [[neurophenomenology]] are labour-intensive. But it represents a more honest framing of what alignment is trying to achieve—promoting flourishing conscious experience, not merely satisfying preferences that may not track what matters.

### The Special Weight of Valence

[[Emotional-consciousness|Emotional consciousness]] research suggests experiential alignment should give special weight to **valence**—the felt positive or negative quality of experience. Valence is not one phenomenal dimension among many; it may be the dimension that matters most for alignment.

The [[emotional-consciousness#valence|hedonic account]] holds that valence is an intrinsic property of experience: the badness of pain is what pain *is*, not something pain represents. If this is correct, then a complete description of preferences, behaviours, and neural states leaves out precisely what matters—the felt goodness and badness that gives experience its significance.

This has practical implications for experiential alignment:

| Approach | What It Targets | The Problem |
|----------|-----------------|-------------|
| **Preference learning** | Revealed choices | Choices don't track felt quality; pain asymbolia patients prefer pain-causing activities |
| **Broad experiential** | Any conscious features | Treats phenomenal unity and suffering as equivalent concerns |
| **Valence-weighted** | Hedonic quality first | Accepts that suffering and enjoyment are not just features of experience but *what makes experience matter* |

[[Emotional-consciousness#Emotional Consciousness and Moral Status|Valence sentientism]]—the view that moral status derives from capacity for suffering and enjoyment—provides the theoretical grounding. If only valenced experience creates interests worth protecting, then alignment systems should prioritise the valence dimension over other phenomenal features.

The research on [[emotional-consciousness#Core Affect and Emotional Construction|core affect]] adds nuance: humans are never affectively neutral. There is always a background felt quality of experience, varying in pleasantness and arousal. A truly aligned system would consider this continuous valenced character, not merely discrete episodes of pleasure and pain. The goal is not preference satisfaction, nor even a list of positive experiences, but supporting lives rich in positive valence across their temporal extent.

## Further Reading

- [[emotional-consciousness]] — The phenomenology of valence and why felt quality matters for alignment
- [[experiential-alignment]] — The alternative framework targeting experiential quality
- [[phenomenal-value-realism]] — The metaethical grounding for consciousness-based value
- [[ethics-of-consciousness]] — The broader ethical framework for consciousness
- [[meaning-of-life]] — the Map's treatment of philosophical approaches to life's meaning
- [[ai-consciousness]] — Why the Map holds that AI systems lack consciousness
- [[voids]] — On the limits of what can be known or mapped
- [[tenets]] — The foundational commitments underlying this analysis

## References

1. Zhi-Xuan, T., Carroll, M., Franklin, M., & Ashton, H. (2024). "Beyond Preferences in AI Alignment." *Philosophical Studies*. https://arxiv.org/abs/2408.16984
2. Russell, S. (2019). *Human Compatible: Artificial Intelligence and the Problem of Control*. Viking.
3. Wolf, S. (2010). *Meaning in Life and Why It Matters*. Princeton University Press.
4. Chalmers, D. (1996). *The Conscious Mind*. Oxford University Press.
