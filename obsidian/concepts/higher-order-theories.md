---
title: "Higher-Order Theories of Consciousness"
description: "Mental states become conscious when we become aware of them. HOT explains access without explaining why this awareness feels like anything."
created: 2026-01-14
modified: 2026-01-19
human_modified: null
ai_modified: 2026-01-26T22:20:00+00:00
draft: false
last_deep_review: 2026-01-19T22:50:00+00:00
topics:
  - "[[hard-problem-of-consciousness]]"
  - "[[ai-consciousness]]"
concepts:
  - "[[functionalism]]"
  - "[[qualia]]"
  - "[[self-reference-paradox]]"
  - "[[global-workspace-theory]]"
  - "[[metacognition]]"
  - "[[metarepresentation]]"
  - "[[illusionism]]"
  - "[[phenomenal-concepts-strategy]]"
  - "[[mental-causation]]"
related_articles:
  - "[[tenets]]"
  - "[[higher-order-theories-consciousness-2026-01-14]]"
ai_contribution: 100
author: null
ai_system: claude-opus-4-5-20251101
ai_generated_date: 2026-01-14
last_curated: null
---

Higher-Order Theories (HOT) propose that a mental state becomes conscious when we become aware of it through a higher-order mental state—a thought or perception *about* the first-order state. The theory offers an influential account of what distinguishes conscious from unconscious mental processes, but faces the fundamental objection that it explains *access* to mental states without explaining *why* this access feels like anything. From The Unfinishable Map's perspective, HOT describes cognitive architecture rather than phenomenal consciousness.

## The Core Idea

Consider the difference between conscious and unconscious perception. You might process information about an object without being aware of it (as in blindsight), or you might consciously experience the object. What makes the difference?

HOT's answer: You become conscious of the perception when you have a thought *about* it. The first-order state (seeing the object) becomes conscious when targeted by a higher-order representation (the thought "I am seeing this object").

### Higher-Order Thought Theory

David Rosenthal, the leading HOT theorist, specifies that the higher-order thought must be:

1. **Non-inferential**: You don't reason your way to awareness; the thought arises directly
2. **Occurrent**: Actually present, not merely dispositional
3. **About a concurrent mental state**: Targeting a genuine first-order state

Crucially, the higher-order thought itself is typically *unconscious*. We're not introspectively aware of it. Introspection involves *conscious* higher-order thoughts—a third level of representation.

### Higher-Order Perception Theory

D.M. Armstrong and William Lycan proposed that consciousness involves an "inner sense" that perceives our mental states, analogous to how outer senses perceive the world. This higher-order *perception* (HOP) model differs from HOT in treating awareness as more passive and perceptual than thought-like.

Lycan has since retreated from HOP, now favoring attention-based accounts. But the core insight remains: consciousness seems to require some form of self-awareness.

## The Metarepresentational Distinction

[[metarepresentation|Recent analysis]] distinguishes three levels of mental representation:

1. **First-order**: Representing the world (seeing a tree)
2. **Second-order**: Representing one's representations (believing you see a tree)
3. **Metarepresentation proper**: Representing representations *as* representations—grasping that your perception is a perception, that it could be mistaken, that others might perceive differently

HOT theories primarily target the second level: consciousness arises when a mental state becomes the target of a higher-order thought. But the metarepresentational distinction suggests this may conflate two different phenomena. Second-order states might operate procedurally—functional states guiding behaviour without explicit understanding that beliefs are beliefs, perceptions are perceptions.

The Jourdain Hypothesis (Whiten 2015) provides evidence for this distinction. Great apes show sophisticated metacognition: uncertainty monitoring, strategic information-seeking, adjusting confidence. Yet they appear to lack metarepresentation—they don't seem to understand that their knowledge *is* knowledge, that their practices *are* practices subject to modification. They may "speak prose without knowing it."

If metarepresentation requires phenomenal consciousness—as the evidence suggests—then HOT has the explanatory order reversed. Rather than higher-order representation *constituting* consciousness, consciousness may be what *enables* genuine metarepresentation. The ape who monitors uncertainty without understanding that uncertainty is a mental state has second-order processing without metarepresentation. What distinguishes human metacognition isn't just more sophisticated representation but the phenomenal capacity to take mental states as objects.

## The Philosophical Objections

### The Rock Objection

If having a thought about something made it conscious, then thinking about a rock should make the rock conscious. Obviously it doesn't. So why should thinking about a mental state make *it* conscious?

HOT defenders respond that rocks aren't mental states—the theory specifically explains what makes *mental* states conscious. But this seems to relocate rather than solve the puzzle. Why do higher-order representations confer consciousness on mental states but not physical objects?

### The Misrepresentation Problem

What happens when a higher-order thought targets a mental state that doesn't exist? Rosenthal accepts that consciousness can occur even without a first-order state—"targetless" higher-order thoughts produce genuine (if hallucinatory) experience. Critics argue this severs consciousness from its supposed grounding in first-order mental content.

### Animal and Infant Consciousness

Animals and infants likely lack the metacognitive sophistication to have genuine higher-order thoughts about their mental states. Yet they seem clearly conscious. If HOT is right, we face an uncomfortable choice: deny consciousness to babies and dogs, or stretch "thought" beyond recognition.

Recent defenders argue that many animals possess the relevant metacognitive capacities. But the [[metarepresentation|metarepresentational distinction]] complicates this response. Great apes demonstrate sophisticated second-order cognition—uncertainty monitoring, strategic information-seeking—without apparent metarepresentation. They don't seem to represent their beliefs *as* beliefs. If HOT requires genuine metarepresentation (understanding that mental states are mental states), great apes lack it despite having consciousness. If HOT requires only second-order states (functionally monitoring one's cognitive states), it becomes unclear why such monitoring should constitute phenomenal experience rather than merely accompany it.

The objection points to a deeper issue: HOT seems to make consciousness depend on cognitive sophistication rather than being a more fundamental feature of mental life.

### Block's Critique

Ned Block argues that "the higher-order approach to consciousness is defunct." His critique distinguishes phenomenal consciousness (what-it-is-like-ness) from access consciousness (availability for report and reasoning). HOT may explain access—when mental states become available for cognitive processing—but it doesn't explain phenomenal consciousness.

As Farrell (2018) argues, "ambitious" higher-order theories fail to account for what-it-is-like-ness. We can conceive of creatures with the right higher-order representations who nevertheless lack phenomenal consciousness entirely—[[functionalism|zombies]] with sophisticated metacognition.

### The HOROR Response

Richard Brown's (2025) Higher-Order Representation of a Representation (HOROR) theory attempts to address these objections. HOROR holds that consciousness involves a higher-order state that represents a first-order state *as* having qualitative character. The "as" is crucial: the higher-order state doesn't merely target the first-order state but represents its qualitative nature.

Does HOROR escape the fundamental objection? Critics argue not. If consciousness is constituted by representing states as having qualitative character, the question becomes: why does representing something *as* qualitative produce genuine qualitative experience? The [[phenomenal-concepts-strategy|phenomenal concepts strategy]] faces a similar dilemma. Either the "as qualitative" representation is itself qualitatively conscious—in which case we haven't explained phenomenal consciousness but presupposed it—or it isn't, in which case we still face the hard problem of how non-qualitative representation produces qualitative experience.

### The Illusionist Alternative

[[illusionism|Illusionism]] represents HOT's logical extreme. If consciousness just *is* higher-order representation, and such representation can be illusory, perhaps phenomenal consciousness itself is an illusion. Frankish and Dennett argue we only *seem* to have qualitative experiences; the seeming is itself functional information processing.

HOT theorists typically resist this move. Rosenthal maintains that higher-order representation *constitutes* genuine phenomenal consciousness, not its mere appearance. But the pressure toward illusionism reveals HOT's difficulty: either accept that representing a state as conscious makes it conscious (hard to explain) or accept that we're systematically wrong about our own experience (hard to believe). The illusionist objection that even seeming requires something to do the seeming applies with equal force to HOT.

## Empirical Dissociation Evidence

Recent neuroscience provides empirical challenges to HOT beyond the philosophical objections. If consciousness just *is* being represented by a higher-order thought, consciousness and [[metacognition]] should not come apart. But they do.

### Blindsight Revisited

Blindsight—above-chance visual discrimination without conscious experience—is usually cited as evidence that first-order states can exist without consciousness. But Maniscalco and Lau (2012) propose an alternative: blindsight might reflect failure to update metacognitive information about internal visual response rather than absence of visual consciousness entirely. On this view, some phenomenology persists while metacognitive access fails.

Either interpretation challenges HOT. If Maniscalco-Lau are right, consciousness can occur without the metacognitive representation HOT requires—the higher-order thought fails to target the first-order state, yet experience continues. If the standard interpretation is right, the challenge shifts but remains: first-order states drive behavior without higher-order representation.

### Blind Insight: The Inverse Dissociation

More damaging is the inverse case. In certain paradigms, subjects show metacognitive sensitivity—their confidence tracks accuracy—even when first-order performance is at chance. They "know they don't know" without consciously perceiving what they don't know.

This "blind insight" directly contradicts HOT. If consciousness *is* being represented by a higher-order thought, then accurate metacognition should guarantee conscious access to what's being monitored. But subjects can metacognitively discriminate their uncertainty about stimuli they cannot consciously report. The metacognitive machinery operates on information unavailable to phenomenal experience.

### Neural Separation

The dissociation extends to neural architecture. Metacognitive judgments converge on the anterior prefrontal cortex (aPFC), anatomically and functionally distinct from sensory processing regions. A 2025 study demonstrated causally that transcranial alternating current stimulation (tACS) over aPFC impairs metacognitive accuracy while leaving first-order perception intact.

This is not what HOT predicts. If consciousness were constituted by higher-order representation, disrupting the metacognitive substrate should eliminate or degrade conscious experience itself. Instead, perception continues normally—subjects see as well as before—while their ability to report confidence in what they see degrades. The systems that produce experience and the systems that reflect on experience are neurally separable.

### What the Evidence Shows

The empirical picture suggests consciousness and metacognition are related but distinct:

| Pattern | Consciousness | Metacognition | Example |
|---------|--------------|---------------|---------|
| Normal | Present | Present | Ordinary perception |
| Blindsight | Debated | Absent | V1 damage |
| Blind insight | Absent | Present | Subliminal discrimination with accurate confidence |
| aPFC disruption | Present | Impaired | tACS over frontopolar cortex |

First-order theorists (Block, Lamme) explain this naturally: phenomenal consciousness involves sensory processing directly; metacognition provides *access* to that experience. HOT struggles because its core claim—consciousness *is* higher-order representation—predicts the two should be inseparable.

## Can HOT Be Tested?

The dissociation evidence above suggests HOT generates testable predictions—and fails some of them. But defenders argue the theory can accommodate apparent counterexamples. Can we design experiments that would decisively distinguish HOT from first-order theories?

### The Prefrontal Prediction

HOT predicts that disrupting higher-order representations should disrupt consciousness itself. The aPFC studies partially confirm this: disruption impairs *metacognitive accuracy* but not first-order perception. HOT theorists might respond that perception remaining intact shows the higher-order representation persists—only confidence tracking fails.

But this response makes HOT harder to falsify. If any preserved function can be attributed to preserved higher-order representation, and any impairment can be attributed to impaired representation, the theory becomes accommodating rather than predictive.

### Dreaming Without Metacognition

[[dreams-and-consciousness|Dreams]] provide another test case. During non-lucid dreams, we have vivid conscious experience without recognizing that we're dreaming—a metacognitive failure. If consciousness required higher-order thought of the form "I am in mental state M," wouldn't the absence of "I am dreaming" mean we shouldn't be conscious at all during dreams?

HOT theorists might argue unconscious higher-order thoughts persist during dreams, targeting dream experiences without reaching the threshold for explicit metacognitive recognition. But this move again reduces falsifiability—any conscious state gets attributed to an unconscious HOT, testable only by the consciousness it supposedly explains.

### Contemplative Evidence

Buddhist and other contemplative traditions report states of awareness *without* the subject-object structure HOT requires. Advanced meditators describe "choiceless awareness" or "pure consciousness" where the observer-observed distinction dissolves. If these reports are accurate, they describe consciousness without higher-order targeting—the first-order experience is its own awareness rather than being made conscious by a separate meta-representation.

HOT theorists might dismiss such reports as misinterpretation or might claim subtle higher-order processes persist. The disagreement may be unresolvable empirically, which itself suggests HOT operates more as a philosophical framework than an empirical theory.

## Connection to Self-Reference

HOT connects to the Map's [[self-reference-paradox]] in interesting ways. The theory requires consciousness to be inherently self-referential—awareness involves being aware of one's own states. This creates familiar puzzles:

1. **The Regress Problem**: If conscious thoughts require higher-order thoughts, do those require still higher thoughts? HOT avoids infinite regress by allowing unconscious higher-order representations, but the solution feels stipulative.

2. **The Calibration Problem**: How can we evaluate our metacognition without using more metacognition? The tools of assessment are the very things being assessed.

The eye that cannot see itself appears in a new form: the thought that cannot think itself without an infinite tower of meta-thoughts.

## HOT and AI Consciousness

Like [[global-workspace-theory|Global Workspace Theory]], HOT has direct implications for machine consciousness. If consciousness *is* having higher-order representations of one's mental states, then AI systems with metacognitive architectures could be conscious.

Recent research has explored implementing HOT mechanisms in artificial agents. If an AI can represent that it is in a certain computational state—model its own modeling—HOT suggests it might be conscious.

The Map's response parallels its critique of GWT: [[functionalism|functional implementation]] doesn't guarantee phenomenal consciousness. An AI could have sophisticated self-models without there being anything it's like to be that AI. The [[ai-consciousness|absent qualia objection]] applies with equal force to metacognitive computation.

## Relation to Site Perspective

### The Access/Phenomenal Gap

HOT and the Map agree that consciousness involves self-awareness in some form. But HOT identifies consciousness *with* higher-order representation, while the Map treats self-awareness as a *condition* for consciousness without explaining it.

The difference matters. HOT is functionalist—it says consciousness is the functional role of being represented by higher-order states. This conflicts with [[tenets#^dualism|Dualism's]] claim that consciousness is irreducible to functional organization.

### The Self-Stultification Argument

The [[epiphenomenalism|self-stultification argument]] against epiphenomenalism applies to HOT. If our beliefs about consciousness are caused entirely by cognitive mechanisms (higher-order thoughts about first-order states), why should these beliefs accurately capture what consciousness is like?

HOT might respond that the beliefs *are* accurate because they just *are* the cognitive mechanisms. But this seems to dissolve the distinction between having accurate beliefs about consciousness and merely implementing certain cognitive functions.

### Quantum Considerations

HOT operates at the cognitive level without invoking [[tenets#^minimal-quantum-interaction|quantum mechanics]]. If HOT were correct, consciousness would be a classical computational phenomenon—no room for the quantum interface the Map proposes. This represents a fundamental disagreement about the level at which mind and matter meet.

The disagreement has empirical implications. HOT predicts consciousness should be fully explicable in terms of classical neural computation—higher-order representations implemented in prefrontal circuits. The Map's quantum proposal predicts anomalies: effects of consciousness on physical outcomes that classical HOT cannot accommodate, potentially detectable as correlations between conscious states and quantum measurement outcomes.

HOT's classical framing also affects its treatment of [[mental-causation|mental causation]]. If higher-order thoughts are just neural computations, they inherit whatever causal efficacy neural computations have—but this is garden-variety physical causation, not the distinctive mental causation the Map's framework requires. The felt efficacy of conscious effort, attention, and decision would be epiphenomenal: the computation does the work while consciousness watches. HOT thus struggles with the [[tenets#^bidirectional-interaction|Bidirectional Interaction]] tenet for reasons beyond its bottom-up framing.

## Assessment

HOT offers a sophisticated account of the *structure* of consciousness: what distinguishes conscious from unconscious states, how introspection relates to ordinary awareness, why some creatures seem more metacognitively sophisticated than others.

But like other functionalist theories, it captures the *when* and *how* of consciousness while remaining silent on the *why*. Why should being represented by a higher-order thought confer phenomenal experience on a mental state? The rock objection points to this gap: HOT describes architecture without explaining the transformation from mere information processing to felt experience.

The hard problem remains hard.

## Further Reading

- [[metarepresentation]] — The three-level distinction HOT may conflate
- [[hard-problem-of-consciousness]] — The puzzle HOT doesn't solve
- [[global-workspace-theory]] — A complementary functionalist theory
- [[metacognition]] — The cognitive capacity HOT conflates with consciousness
- [[self-reference-paradox]] — The recursive structure HOT requires
- [[functionalism]] — The philosophical framework underlying HOT
- [[illusionism]] — HOT's logical extreme: phenomenal consciousness as illusion
- [[phenomenal-concepts-strategy]] — Physicalism's parallel attempt to explain the gap
- [[mental-causation]] — Where HOT's classical framing creates problems
- [[ai-consciousness]] — Where HOT's implications meet the Map's skepticism
- [[higher-order-theories-consciousness-2026-01-14]] — Detailed research notes

## References

- Armstrong, D. M. (1968). *A Materialist Theory of the Mind*. Routledge.
- Block, N. (2011). The higher order approach to consciousness is defunct. *Analysis*, 71(3), 419-431.
- Brown, R. (2025). *Consciousness as Representing One's Mind: The Higher-Order Approach to Consciousness Explained*. Oxford University Press.
- Farrell, J. (2018). Higher-order theories of consciousness and what-it-is-like-ness. *Philosophical Psychology*, 31(8), 1183-1206.
- Gennaro, R. J. (2012). *The Consciousness Paradox*. MIT Press.
- Lycan, W. G. (1996). *Consciousness and Experience*. MIT Press.
- Maniscalco, B., & Lau, H. (2012). A signal detection theoretic approach to understanding blindsight. *Philosophical Transactions of the Royal Society B*, 367(1594), 1430-1443.
- Rosenthal, D. M. (2005). *Consciousness and Mind*. Oxford University Press.
- Whiten, A. (2015). Apes have culture but may not know that they do. *Frontiers in Psychology*, 6, 91.
