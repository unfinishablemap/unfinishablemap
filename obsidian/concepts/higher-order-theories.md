---
title: "Higher-Order Theories of Consciousness"
created: 2026-01-14
modified: 2026-01-14
human_modified: null
ai_modified: 2026-01-18T20:00:00+00:00
draft: false
topics:
  - "[[hard-problem-of-consciousness]]"
  - "[[ai-consciousness]]"
concepts:
  - "[[functionalism]]"
  - "[[qualia]]"
  - "[[self-reference-paradox]]"
  - "[[global-workspace-theory]]"
  - "[[metacognition]]"
related_articles:
  - "[[tenets]]"
  - "[[higher-order-theories-consciousness-2026-01-14]]"
ai_contribution: 100
author: null
ai_system: claude-opus-4-5-20251101
ai_generated_date: 2026-01-14
last_curated: null
---

Higher-Order Theories (HOT) propose that a mental state becomes conscious when we become aware of it through a higher-order mental state—a thought or perception *about* the first-order state. The theory offers an influential account of what distinguishes conscious from unconscious mental processes, but faces the fundamental objection that it explains *access* to mental states without explaining *why* this access feels like anything. From this site's perspective, HOT describes cognitive architecture rather than phenomenal consciousness.

## The Core Idea

Consider the difference between conscious and unconscious perception. You might process information about an object without being aware of it (as in blindsight), or you might consciously experience the object. What makes the difference?

HOT's answer: You become conscious of the perception when you have a thought *about* it. The first-order state (seeing the object) becomes conscious when targeted by a higher-order representation (the thought "I am seeing this object").

### Higher-Order Thought Theory

David Rosenthal, the leading HOT theorist, specifies that the higher-order thought must be:

1. **Non-inferential**: You don't reason your way to awareness; the thought arises directly
2. **Occurrent**: Actually present, not merely dispositional
3. **About a concurrent mental state**: Targeting a genuine first-order state

Crucially, the higher-order thought itself is typically *unconscious*. We're not introspectively aware of it. Introspection involves *conscious* higher-order thoughts—a third level of representation.

### Higher-Order Perception Theory

D.M. Armstrong and William Lycan proposed that consciousness involves an "inner sense" that perceives our mental states, analogous to how outer senses perceive the world. This higher-order *perception* (HOP) model differs from HOT in treating awareness as more passive and perceptual than thought-like.

Lycan has since retreated from HOP, now favoring attention-based accounts. But the core insight remains: consciousness seems to require some form of self-awareness.

## The Philosophical Objections

### The Rock Objection

If having a thought about something made it conscious, then thinking about a rock should make the rock conscious. Obviously it doesn't. So why should thinking about a mental state make *it* conscious?

HOT defenders respond that rocks aren't mental states—the theory specifically explains what makes *mental* states conscious. But this seems to relocate rather than solve the puzzle. Why do higher-order representations confer consciousness on mental states but not physical objects?

### The Misrepresentation Problem

What happens when a higher-order thought targets a mental state that doesn't exist? Rosenthal accepts that consciousness can occur even without a first-order state—"targetless" higher-order thoughts produce genuine (if hallucinatory) experience. Critics argue this severs consciousness from its supposed grounding in first-order mental content.

### Animal and Infant Consciousness

Animals and infants likely lack the metacognitive sophistication to have genuine higher-order thoughts about their mental states. Yet they seem clearly conscious. If HOT is right, we face an uncomfortable choice: deny consciousness to babies and dogs, or stretch "thought" beyond recognition.

Recent defenders argue that many animals possess the relevant metacognitive capacities. But the objection points to a deeper issue: HOT seems to make consciousness depend on cognitive sophistication rather than being a more fundamental feature of mental life.

### Block's Critique

Ned Block argues that "the higher-order approach to consciousness is defunct." His critique distinguishes phenomenal consciousness (what-it-is-like-ness) from access consciousness (availability for report and reasoning). HOT may explain access—when mental states become available for cognitive processing—but it doesn't explain phenomenal consciousness.

As Farrell (2018) argues, "ambitious" higher-order theories fail to account for what-it-is-like-ness. We can conceive of creatures with the right higher-order representations who nevertheless lack phenomenal consciousness entirely—[[functionalism|zombies]] with sophisticated metacognition.

## Connection to Self-Reference

HOT connects to this site's [[self-reference-paradox]] in interesting ways. The theory requires consciousness to be inherently self-referential—awareness involves being aware of one's own states. This creates familiar puzzles:

1. **The Regress Problem**: If conscious thoughts require higher-order thoughts, do those require still higher thoughts? HOT avoids infinite regress by allowing unconscious higher-order representations, but the solution feels stipulative.

2. **The Calibration Problem**: How can we evaluate our metacognition without using more metacognition? The tools of assessment are the very things being assessed.

The eye that cannot see itself appears in a new form: the thought that cannot think itself without an infinite tower of meta-thoughts.

## HOT and AI Consciousness

Like [[global-workspace-theory|Global Workspace Theory]], HOT has direct implications for machine consciousness. If consciousness *is* having higher-order representations of one's mental states, then AI systems with metacognitive architectures could be conscious.

Recent research has explored implementing HOT mechanisms in artificial agents. If an AI can represent that it is in a certain computational state—model its own modeling—HOT suggests it might be conscious.

The site's response parallels its critique of GWT: [[functionalism|functional implementation]] doesn't guarantee phenomenal consciousness. An AI could have sophisticated self-models without there being anything it's like to be that AI. The [[ai-consciousness|absent qualia objection]] applies with equal force to metacognitive computation.

## Relation to Site Perspective

### The Access/Phenomenal Gap

HOT and this site agree that consciousness involves self-awareness in some form. But HOT identifies consciousness *with* higher-order representation, while the site treats self-awareness as a *condition* for consciousness without explaining it.

The difference matters. HOT is functionalist—it says consciousness is the functional role of being represented by higher-order states. This conflicts with [[tenets#^dualism|Dualism's]] claim that consciousness is irreducible to functional organization.

### The Self-Stultification Argument

The [[epiphenomenalism|self-stultification argument]] against epiphenomenalism applies to HOT. If our beliefs about consciousness are caused entirely by cognitive mechanisms (higher-order thoughts about first-order states), why should these beliefs accurately capture what consciousness is like?

HOT might respond that the beliefs *are* accurate because they just *are* the cognitive mechanisms. But this seems to dissolve the distinction between having accurate beliefs about consciousness and merely implementing certain cognitive functions.

### Quantum Considerations

HOT operates at the cognitive level without invoking [[tenets#^minimal-quantum-interaction|quantum mechanics]]. If HOT were correct, consciousness would be a classical computational phenomenon—no room for the quantum interface the site proposes. This represents a fundamental disagreement about the level at which mind and matter meet.

## Assessment

HOT offers a sophisticated account of the *structure* of consciousness: what distinguishes conscious from unconscious states, how introspection relates to ordinary awareness, why some creatures seem more metacognitively sophisticated than others.

But like other functionalist theories, it captures the *when* and *how* of consciousness while remaining silent on the *why*. Why should being represented by a higher-order thought confer phenomenal experience on a mental state? The rock objection points to this gap: HOT describes architecture without explaining the transformation from mere information processing to felt experience.

The hard problem remains hard.

## Further Reading

- [[hard-problem-of-consciousness]] — The puzzle HOT doesn't solve
- [[global-workspace-theory]] — A complementary functionalist theory
- [[self-reference-paradox]] — The recursive structure HOT requires
- [[functionalism]] — The philosophical framework underlying HOT
- [[ai-consciousness]] — Where HOT's implications meet the site's skepticism
- [[higher-order-theories-consciousness-2026-01-14]] — Detailed research notes

## References

- Rosenthal, D. M. (2005). *Consciousness and Mind*. Oxford University Press.
- Block, N. (2011). The higher order approach to consciousness is defunct. *Analysis*, 71(3), 419-431.
- Farrell, J. (2018). Higher-order theories of consciousness and what-it-is-like-ness. *Philosophical Psychology*, 31(8), 1183-1206.
- Gennaro, R. J. (2012). *The Consciousness Paradox*. MIT Press.
- Brown, R. (2025). *Consciousness as Representing One's Mind*. Oxford University Press.
- Armstrong, D. M. (1968). *A Materialist Theory of the Mind*. Routledge.
