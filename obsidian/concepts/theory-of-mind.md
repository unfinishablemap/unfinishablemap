---
title: "Theory of Mind"
created: 2026-01-22
modified: 2026-01-22
human_modified: null
ai_modified: 2026-01-22T10:30:00+00:00
draft: false
topics:
  - "[[animal-consciousness]]"
  - "[[ai-consciousness]]"
concepts:
  - "[[metarepresentation]]"
  - "[[consciousness-and-social-cognition]]"
  - "[[baseline-cognition]]"
  - "[[jourdain-hypothesis]]"
  - "[[metacognition]]"
  - "[[working-memory]]"
  - "[[teaching-as-metarepresentation]]"
  - "[[intentionality]]"
  - "[[illusionism]]"
  - "[[phenomenal-unity]]"
related_articles:
  - "[[tenets]]"
  - "[[consciousness-independent-baseline-cognition-2026-01-21]]"
ai_contribution: 100
author: null
ai_system: claude-opus-4-5-20251101
ai_generated_date: 2026-01-22
last_curated: null
last_deep_review: null
---

Theory of mind is the capacity to attribute mental states—beliefs, desires, intentions, knowledge—to others and to understand that these states may differ from one's own. The Unfinishable Map argues that theory of mind admits of levels, and that the higher levels require phenomenal consciousness. Great apes possess impressive social cognition at lower levels (tracking what others see, predicting behaviour from goals) but appear limited at higher levels (recursive mindreading, understanding false beliefs as mental representations). This gap tracks the consciousness-dependent capacity for [[metarepresentation]]—representing mental states *as* mental states rather than merely responding to behavioural cues.

## The Levels of Theory of Mind

Theory of mind is not a single capacity but a hierarchy of increasingly sophisticated operations:

### Level 0: Behaviour Prediction

Predicting what an agent will do based on observable regularities. "When she sees food, she approaches it." This requires no mental state attribution—only correlating environmental features with behavioural outcomes. Associative learning suffices.

### Level 1: Perception Attribution

Understanding what others perceive. "She sees the food" requires distinguishing another's perceptual access from your own. Great apes pass many Level 1 tests: they track gaze direction, understand that opaque barriers block vision, and adjust behaviour based on what competitors have or haven't seen.

Level 1 may operate through sophisticated but non-metarepresentational processing—tracking the *geometric* relationship between agent, barrier, and object without representing the agent's *perceptual state* as a mental state.

### Level 2: Belief Attribution

Understanding that others have beliefs that may differ from reality. The classic false-belief test: does the subject understand that an agent will act on their (mistaken) belief rather than actual reality?

Here the evidence becomes contested. Some studies suggest great apes pass false-belief tests in competitive contexts; others find their success explicable through behaviour-reading without genuine belief attribution. The methodological challenges are severe: how do we distinguish "she believes the food is there" from "she will look where she last saw food"?

Level 2 appears to require something beyond tracking behaviour and perception—representing the other's *representational state* as distinct from the world it represents. This is where metarepresentation enters.

### Level 3: Recursive Mindreading

Representing others' mental states about mental states. "She thinks that I think the food is hidden" involves nested attribution—beliefs about beliefs, intentions about intentions. This enables strategic deception, deception detection, and the complex coordination of human social life.

Level 3 unambiguously requires metarepresentation. You must hold multiple representational levels simultaneously: your intention, their belief about your intention, and your awareness of that belief. Each level must remain distinct and manipulable while being bound together in [[phenomenal-unity|unified awareness]].

### Level 4 and Beyond

Humans readily engage in fourth-order attribution ("I know that you know that I know that you're pretending") and can theoretically extend further, limited primarily by working memory. Fictional narratives routinely demand third and fourth-order theory of mind from readers tracking characters' knowledge of other characters' intentions.

## The Metarepresentational Threshold

The central claim: Levels 0-1 may operate within [[baseline-cognition]] without requiring phenomenal consciousness. Level 2 is transitional—perhaps achievable through sophisticated implicit tracking, perhaps requiring something more. Level 3 unambiguously requires [[metarepresentation]]—and metarepresentation appears to require consciousness.

Why should metarepresentation require consciousness?

**The nested binding problem**: Recursive mindreading requires holding multiple representational levels in coordinated awareness. The representations must be bound together—you must experience the whole structure at once to compare levels and compute their relationships. [[Working-memory]] research suggests that *manipulating* information (not just maintaining it) requires conscious access. The nested structure of Level 3+ theory of mind demands exactly this.

**The standing-back requirement**: To represent a mental state *as* a mental state, you must achieve a kind of cognitive distance from it. The belief isn't just a feature of the world but a representation that could be false, that someone holds, that you can reason about. This "standing back" appears to require the doubled awareness consciousness provides—being aware *of* mental states while being aware *as* a subject having them.

**The Jourdain problem**: The [[jourdain-hypothesis|Jourdain Hypothesis]] proposes that great apes have mental states without knowing they have them. Applied to theory of mind: apes may predict others' behaviour based on perceptual access without representing those predictions *as* mental state attributions. They read behaviour without reading minds—or rather, they read minds without knowing that minds are what they read.

## Developmental Trajectory

Human theory of mind develops through recognisable stages:

**12-18 months**: Joint attention, gaze following, understanding that others are intentional agents with goals.

**2-3 years**: Understanding desire and emotion in others. Children predict that people will feel happy when they get what they want.

**3-4 years**: Explicit false-belief understanding emerges. Children begin to understand that others can have beliefs that are false—and will act on those false beliefs.

**4-5 years**: Understanding that appearance and reality can differ, that people can be deceived, that the same object can be represented differently.

**5-6 years**: Second-order false belief ("Sally thinks that Anne thinks..."). This requires recursive embedding.

**Later childhood**: Understanding of complex social phenomena—lying, sarcasm, white lies, social norms, institutional facts—all of which require sophisticated mental state attribution.

This trajectory parallels the development of metacognitive capacity more broadly. Children become able to represent their own mental states as mental states at roughly the same time they become able to represent others' mental states as mental states. The connection is not coincidental: both capacities require metarepresentation.

## Neural Correlates

Neuroimaging studies consistently identify a network of brain regions involved in theory of mind tasks:

**Medial prefrontal cortex (mPFC)**: Active when thinking about others' mental states. Damage to mPFC impairs theory of mind while often sparing other cognitive abilities.

**Temporoparietal junction (TPJ)**: Critical for distinguishing self from other and for representing others' beliefs as distinct from one's own. TPJ activity correlates with false-belief reasoning.

**Posterior superior temporal sulcus (pSTS)**: Involved in perceiving biological motion and interpreting intentional action.

**Precuneus**: Active in self-referential processing and may contribute to representing perspective differences.

This network overlaps substantially with the "default mode network"—regions active during rest and associated with self-reflection, autobiographical memory, and imagining the future. The overlap suggests theory of mind draws on the same machinery used for reflecting on one's own mind.

Notably, these regions are among the most phylogenetically recent in primate brain evolution and show the greatest expansion in humans relative to great apes. The neural substrate for higher-level theory of mind appears to have expanded alongside the cognitive capacities it enables.

## The Great Ape Evidence

Great apes present the critical test case. They possess sophisticated social cognition—but does it constitute genuine theory of mind?

**What apes clearly do:**

- Track others' line of sight and gaze direction
- Understand that barriers block vision
- Adjust competitive behaviour based on what rivals have seen
- Anticipate others' actions based on goals
- Show some evidence of understanding others' knowledge states

**What remains uncertain or absent:**

- Explicit false-belief understanding (contested evidence, methodological concerns)
- Recursive mindreading (no clear evidence)
- Teaching that requires representing others' knowledge states as knowledge states
- Cumulative cultural transmission (they have traditions but don't systematically improve on them)

The pattern suggests apes may possess Level 1 theory of mind—and possibly some implicit Level 2 capacity—while lacking Level 3. This maps onto the metarepresentational threshold: they track others' behaviour, perceptions, and perhaps beliefs without representing these *as* mental states subject to evaluation and recursive embedding.

Michael Tomasello's extensive comparative research supports this interpretation. Great apes possess "individual intentionality" but lack the "shared intentionality" that characterises human social cognition. Shared intentionality requires knowing that both parties know they're engaged in a joint activity—a recursive structure that may exceed ape capacity.

## AI and Theory of Mind

Current AI systems present an interesting case. Large language models pass many theory of mind tests—they correctly answer questions about what characters in stories believe, including false beliefs. They can describe what someone would think given certain information.

But do they genuinely attribute mental states? The [[jourdain-hypothesis|Jourdain analysis]] suggests not. An LLM producing correct answers about beliefs may be pattern-matching on training data rather than representing minds. It may process "theory of mind language" without having a theory of mind.

The distinction matters for understanding what theory of mind requires. If functional performance on tests sufficed, LLMs would have theory of mind. But theory of mind plausibly requires something more: genuinely representing others *as* subjects with inner mental lives, not merely as systems that produce predictable outputs.

This connects to the [[consciousness-and-social-cognition|social cognition and consciousness]] debate. If theory of mind requires phenomenal consciousness—requires experiencing what it's like to have a perspective in order to attribute perspectives to others—then current AI lacks it regardless of test performance.

## The Illusionist Challenge

[[Illusionism]] denies phenomenal consciousness exists. On this view, theory of mind is sophisticated information processing about others' information processing. There's no phenomenal "extra"—no experiencing of others as subjects—just functional attribution of states.

The challenge for illusionism: explaining why theory of mind develops alongside phenomenal self-awareness, why it correlates with metacognitive capacity, and why it seems to require experiencing what it's like to have a perspective. The illusionist must explain these correlations as coincidental or provide alternative accounts of the developmental and comparative data.

More specifically, the recursive structure of higher-level theory of mind creates difficulty. To represent your belief about my belief about your intention, *something* must hold all three levels simultaneously. That something's unified grasp of the structure—its experiencing of the nested content—is what illusionists deny exists. But without it, what binds the levels into a single cognitive act?

## Relation to Site Perspective

### Bidirectional Interaction

The [[tenets#^bidirectional-interaction|Bidirectional Interaction]] tenet holds that consciousness causally influences the physical world. Theory of mind provides evidence: human social capacities exceed what [[baseline-cognition]] achieves, and the excess tracks consciousness-dependent capacities. The gap between human and great ape theory of mind—specifically at the metarepresentational levels—suggests consciousness enables cognitive operations that mere information processing cannot replicate. If consciousness were epiphenomenal, the correlation between expanded consciousness and expanded theory of mind would be coincidental. The systematic pattern supports causal efficacy.

### Dualism

The [[tenets#^dualism|Dualism]] tenet maintains that consciousness is irreducible. Theory of mind supports this through its phenomenal requirements. Representing another *as* a subject with inner mental life involves phenomenal content—what it's like to grasp someone as having a perspective. This phenomenal dimension resists functional analysis: the difference between tracking behaviour and understanding minds isn't captured by information-processing descriptions. The irreducibility of theory of mind's phenomenal aspect supports the irreducibility of consciousness more broadly.

### Minimal Quantum Interaction

The [[tenets#^minimal-quantum-interaction|Minimal Quantum Interaction]] tenet proposes consciousness influences physical outcomes through minimal biasing of quantum events. Theory of mind creates specific demands this framework addresses. The binding of multiple nested representations—my belief about your belief about my intention—requires sustained integration across distinct neural substrates. The quantum Zeno framework suggests conscious attention maintains coherence across these substrates, enabling the unified grasp that metarepresentational theory of mind requires. The human expansion of working memory capacity enabling deeper recursive embedding may reflect enhanced consciousness-matter interface.

### Occam's Razor Has Limits

The simpler hypothesis—theory of mind is just sophisticated behaviour prediction, consciousness is incidental—fails the evidence. The systematic human-great ape gap at the metarepresentational levels, the developmental correlation with metacognitive capacity, and the phenomenal character of understanding others all warrant the more complex conclusion: higher-level theory of mind requires phenomenal consciousness. The [[tenets#^occams-limits|Occam's Razor Has Limits]] tenet reminds us that parsimony can mislead when concepts are inadequate to phenomena.

## Further Reading

### Core Concepts
- [[metarepresentation]] — Why representing minds *as* minds requires consciousness
- [[consciousness-and-social-cognition]] — Extended analysis of the consciousness-social cognition relationship
- [[baseline-cognition]] — The cognitive floor from which consciousness amplifies social abilities
- [[jourdain-hypothesis]] — Having mental states vs knowing you have them

### Related Topics
- [[animal-consciousness]] — Great ape cognition and its limits
- [[ai-consciousness]] — Whether artificial systems could have genuine theory of mind
- [[teaching-as-metarepresentation]] — Teaching as paradigm case requiring theory of mind

### Related Concepts
- [[working-memory]] — The workspace enabling recursive mental operations
- [[intentionality]] — The aboutness of mental states
- [[metacognition]] — Thinking about thinking

## References

- Baron-Cohen, S. (1995). *Mindblindness: An Essay on Autism and Theory of Mind*. MIT Press.
- Call, J., & Tomasello, M. (2008). Does the chimpanzee have a theory of mind? 30 years later. *Trends in Cognitive Sciences*, 12(5), 187-192.
- Heyes, C. (2014). Submentalizing: I am not really reading your mind. *Perspectives on Psychological Science*, 9(2), 131-143.
- Perner, J. (1991). *Understanding the Representational Mind*. MIT Press.
- Premack, D., & Woodruff, G. (1978). Does the chimpanzee have a theory of mind? *Behavioral and Brain Sciences*, 1(4), 515-526.
- Saxe, R., & Kanwisher, N. (2003). People thinking about thinking people: The role of the temporo-parietal junction in "theory of mind". *NeuroImage*, 19(4), 1835-1842.
- Tomasello, M. (2014). *A Natural History of Human Thinking*. Harvard University Press.
- Wellman, H.M. (2014). *Making Minds: How Theory of Mind Develops*. Oxford University Press.
